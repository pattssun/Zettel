<html>
    <head>
        <meta charset="utf-8">
        
            <script>function neighbourhoodHighlight(params) {
  // console.log("in nieghbourhoodhighlight");
  allNodes = nodes.get({ returnType: "Object" });
  // originalNodes = JSON.parse(JSON.stringify(allNodes));
  // if something is selected:
  if (params.nodes.length > 0) {
    highlightActive = true;
    var i, j;
    var selectedNode = params.nodes[0];
    var degrees = 2;

    // mark all nodes as hard to read.
    for (let nodeId in allNodes) {
      // nodeColors[nodeId] = allNodes[nodeId].color;
      allNodes[nodeId].color = "rgba(200,200,200,0.5)";
      if (allNodes[nodeId].hiddenLabel === undefined) {
        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }
    var connectedNodes = network.getConnectedNodes(selectedNode);
    var allConnectedNodes = [];

    // get the second degree nodes
    for (i = 1; i < degrees; i++) {
      for (j = 0; j < connectedNodes.length; j++) {
        allConnectedNodes = allConnectedNodes.concat(
          network.getConnectedNodes(connectedNodes[j])
        );
      }
    }

    // all second degree nodes get a different color and their label back
    for (i = 0; i < allConnectedNodes.length; i++) {
      // allNodes[allConnectedNodes[i]].color = "pink";
      allNodes[allConnectedNodes[i]].color = "rgba(150,150,150,0.75)";
      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[allConnectedNodes[i]].label =
          allNodes[allConnectedNodes[i]].hiddenLabel;
        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // all first degree nodes get their own color and their label back
    for (i = 0; i < connectedNodes.length; i++) {
      // allNodes[connectedNodes[i]].color = undefined;
      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];
      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[connectedNodes[i]].label =
          allNodes[connectedNodes[i]].hiddenLabel;
        allNodes[connectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // the main node gets its own color and its label back.
    // allNodes[selectedNode].color = undefined;
    allNodes[selectedNode].color = nodeColors[selectedNode];
    if (allNodes[selectedNode].hiddenLabel !== undefined) {
      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;
      allNodes[selectedNode].hiddenLabel = undefined;
    }
  } else if (highlightActive === true) {
    // console.log("highlightActive was true");
    // reset all nodes
    for (let nodeId in allNodes) {
      // allNodes[nodeId].color = "purple";
      allNodes[nodeId].color = nodeColors[nodeId];
      // delete allNodes[nodeId].color;
      if (allNodes[nodeId].hiddenLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;
        allNodes[nodeId].hiddenLabel = undefined;
      }
    }
    highlightActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    // console.log("Nothing was selected");
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        // allNodes[nodeId].color = {};
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function filterHighlight(params) {
  allNodes = nodes.get({ returnType: "Object" });
  // if something is selected:
  if (params.nodes.length > 0) {
    filterActive = true;
    let selectedNodes = params.nodes;

    // hiding all nodes and saving the label
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = true;
      if (allNodes[nodeId].savedLabel === undefined) {
        allNodes[nodeId].savedLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }

    for (let i=0; i < selectedNodes.length; i++) {
      allNodes[selectedNodes[i]].hidden = false;
      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {
        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;
        allNodes[selectedNodes[i]].savedLabel = undefined;
      }
    }

  } else if (filterActive === true) {
    // reset all nodes
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = false;
      if (allNodes[nodeId].savedLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].savedLabel;
        allNodes[nodeId].savedLabel = undefined;
      }
    }
    filterActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function selectNode(nodes) {
  network.selectNodes(nodes);
  neighbourhoodHighlight({ nodes: nodes });
  return nodes;
}

function selectNodes(nodes) {
  network.selectNodes(nodes);
  filterHighlight({nodes: nodes});
  return nodes;
}

function highlightFilter(filter) {
  let selectedNodes = []
  let selectedProp = filter['property']
  if (filter['item'] === 'node') {
    let allNodes = nodes.get({ returnType: "Object" });
    for (let nodeId in allNodes) {
      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {
        selectedNodes.push(nodeId)
      }
    }
  }
  else if (filter['item'] === 'edge'){
    let allEdges = edges.get({returnType: 'object'});
    // check if the selected property exists for selected edge and select the nodes connected to the edge
    for (let edge in allEdges) {
      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {
        selectedNodes.push(allEdges[edge]['from'])
        selectedNodes.push(allEdges[edge]['to'])
      }
    }
  }
  selectNodes(selectedNodes)
}</script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
            
            
            
            
            

        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 1200px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             
             #config {
                 float: left;
                 width: 400px;
                 height: 600px;
             }
             

             
             /* position absolute is important and the container has to be relative or absolute as well. */
          div.popup {
                 position:absolute;
                 top:0px;
                 left:0px;
                 display:none;
                 background-color:#f5f4ed;
                 -moz-border-radius: 3px;
                 -webkit-border-radius: 3px;
                 border-radius: 3px;
                 border: 1px solid #808074;
                 box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);
          }

          /* hide the original tooltip */
          .vis-tooltip {
            display:none;
          }
             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        
            <div id="config"></div>
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"group": 1, "id": "Large language model", "label": "Large language model", "level": 1, "name": "Large language model", "node_count": 0, "processed": 2, "shape": "dot", "size": 10, "title": "0. \u003ca href=\u0027https://en.wikipedia.org/wiki/Large_language_model\u0027 target=\u0027_blank\u0027\u003eLarge language model\u003c/a\u003e\u003cbr /\u003eA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\u003cbr /\u003e[200, G1, L1, PR]", "wikipedia_canonical": "Large_language_model", "wikipedia_content": "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.", "wikipedia_link": "https://en.wikipedia.org/wiki/Large_language_model", "wikipedia_normalized": "Large language model", "wikipedia_resp_code": 200}, {"group": 2, "id": "Transformer (machine learning model)", "label": "Transformer (machine learning model)", "level": 2, "name": "Transformer (machine learning model)", "node_count": 1, "processed": 2, "shape": "dot", "size": 10, "title": "1. \u003ca href=\u0027https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\u0027 target=\u0027_blank\u0027\u003eTransformer (machine learning model)\u003c/a\u003e \u2192 \u003ca href=\u0027https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\u0027 target=\u0027_blank\u0027\u003eTransformer (deep learning architecture)\u003c/a\u003e\u003cbr /\u003eA transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal \u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "Transformer_(deep_learning_architecture)", "wikipedia_content": "A transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal ", "wikipedia_link": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)", "wikipedia_normalized": "Transformer (deep learning architecture)", "wikipedia_resp_code": 200}, {"group": 2, "id": "BERT (language model)", "label": "BERT (language model)", "level": 2, "name": "BERT (language model)", "node_count": 2, "processed": 2, "shape": "dot", "size": 10, "title": "2. \u003ca href=\u0027https://en.wikipedia.org/wiki/BERT_(language_model)\u0027 target=\u0027_blank\u0027\u003eBERT (language model)\u003c/a\u003e\u003cbr /\u003e \nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "BERT_(language_model)", "wikipedia_content": " \nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"", "wikipedia_link": "https://en.wikipedia.org/wiki/BERT_(language_model)", "wikipedia_normalized": "BERT (language model)", "wikipedia_resp_code": 200}, {"group": 2, "id": "GPT-3", "label": "GPT-3", "level": 2, "name": "GPT-3 (language model)", "node_count": 3, "processed": 2, "shape": "dot", "size": 10, "title": "3. \u003ca href=\u0027https://en.wikipedia.org/wiki/GPT-3\u0027 target=\u0027_blank\u0027\u003eGPT-3\u003c/a\u003e\u003cbr /\u003eGenerative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "GPT-3", "wikipedia_content": "Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre", "wikipedia_link": "https://en.wikipedia.org/wiki/GPT-3", "wikipedia_normalized": "GPT-3", "wikipedia_resp_code": 200}, {"group": 2, "id": "Natural language processing", "label": "Natural language processing", "level": 2, "name": "Natural language processing", "node_count": 4, "processed": 2, "shape": "dot", "size": 10, "title": "4. \u003ca href=\u0027https://en.wikipedia.org/wiki/Natural_language_processing\u0027 target=\u0027_blank\u0027\u003eNatural language processing\u003c/a\u003e\u003cbr /\u003eNatural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the languag\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "Natural_language_processing", "wikipedia_content": "Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the languag", "wikipedia_link": "https://en.wikipedia.org/wiki/Natural_language_processing", "wikipedia_normalized": "Natural language processing", "wikipedia_resp_code": 200}, {"group": 2, "id": "Deep learning", "label": "Deep learning", "level": 2, "name": "Deep learning", "node_count": 5, "processed": 2, "shape": "dot", "size": 10, "title": "5. \u003ca href=\u0027https://en.wikipedia.org/wiki/Deep_learning\u0027 target=\u0027_blank\u0027\u003eDeep learning\u003c/a\u003e\u003cbr /\u003eDeep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "Deep_learning", "wikipedia_content": "Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.", "wikipedia_link": "https://en.wikipedia.org/wiki/Deep_learning", "wikipedia_normalized": "Deep learning", "wikipedia_resp_code": 200}, {"group": 3, "id": "Semantic analysis", "label": "Semantic analysis", "level": 3, "name": "Semantic analysis", "node_count": 20, "processed": 2, "shape": "dot", "size": 10, "title": "20. \u003ca href=\u0027https://en.wikipedia.org/wiki/Semantic_analysis\u0027 target=\u0027_blank\u0027\u003eSemantic analysis\u003c/a\u003e\u003cbr /\u003eSemantic analysis may refer to:\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Semantic_analysis", "wikipedia_content": "Semantic analysis may refer to:", "wikipedia_link": "https://en.wikipedia.org/wiki/Semantic_analysis", "wikipedia_normalized": "Semantic analysis", "wikipedia_resp_code": 200}, {"group": 3, "id": "Text mining", "label": "Text mining", "level": 3, "name": "Text mining", "node_count": 18, "processed": 2, "shape": "dot", "size": 10, "title": "18. \u003ca href=\u0027https://en.wikipedia.org/wiki/Text_mining\u0027 target=\u0027_blank\u0027\u003eText mining\u003c/a\u003e\u003cbr /\u003eText mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Text_mining", "wikipedia_content": "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.", "wikipedia_link": "https://en.wikipedia.org/wiki/Text_mining", "wikipedia_normalized": "Text mining", "wikipedia_resp_code": 200}, {"group": 3, "id": "Computational linguistics", "label": "Computational linguistics", "level": 3, "name": "Computational linguistics", "node_count": 17, "processed": 2, "shape": "dot", "size": 10, "title": "17. \u003ca href=\u0027https://en.wikipedia.org/wiki/Computational_linguistics\u0027 target=\u0027_blank\u0027\u003eComputational linguistics\u003c/a\u003e\u003cbr /\u003eComputational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Computational_linguistics", "wikipedia_content": "Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.", "wikipedia_link": "https://en.wikipedia.org/wiki/Computational_linguistics", "wikipedia_normalized": "Computational linguistics", "wikipedia_resp_code": 200}, {"group": 500, "id": "T5 (text-to-text model)", "label": "T5 (text-to-text model)", "level": 3, "name": "T5 (Text-to-Text Transfer Transformer)", "node_count": 14, "processed": 2, "shape": "dot", "size": 10, "title": "14. \u003ca href=\u0027https://en.wikipedia.org/wiki/T5_(text-to-text_model)\u0027 target=\u0027_blank\u0027\u003eT5 (text-to-text model)\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L3, PR]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/T5_(text-to-text_model)", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 500, "id": "XLNet", "label": "XLNet", "level": 3, "name": "XLNet", "node_count": 13, "processed": 2, "shape": "dot", "size": 10, "title": "13. \u003ca href=\u0027https://en.wikipedia.org/wiki/XLNet\u0027 target=\u0027_blank\u0027\u003eXLNet\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L3, PR]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/XLNet", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 500, "id": "RoBERTa", "label": "RoBERTa", "level": 3, "name": "RoBERTa", "node_count": 15, "processed": 2, "shape": "dot", "size": 10, "title": "15. \u003ca href=\u0027https://en.wikipedia.org/wiki/RoBERTa\u0027 target=\u0027_blank\u0027\u003eRoBERTa\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L3, PR]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/RoBERTa", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 3, "id": "Neural network", "label": "Neural network", "level": 3, "name": "Neural Network", "node_count": 6, "processed": 2, "shape": "dot", "size": 10, "title": "6. \u003ca href=\u0027https://en.wikipedia.org/wiki/Neural_network\u0027 target=\u0027_blank\u0027\u003eNeural network\u003c/a\u003e\u003cbr /\u003eA neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \u2013 a population of nerve cells connected by synapses.\nIn machine learning, an arti\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Neural_network", "wikipedia_content": "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \u2013 a population of nerve cells connected by synapses.\nIn machine learning, an arti", "wikipedia_link": "https://en.wikipedia.org/wiki/Neural_network", "wikipedia_normalized": "Neural network", "wikipedia_resp_code": 200}, {"group": 3, "id": "Recurrent neural network", "label": "Recurrent neural network", "level": 3, "name": "Recurrent Neural Network (RNN)", "node_count": 7, "processed": 2, "shape": "dot", "size": 10, "title": "7. \u003ca href=\u0027https://en.wikipedia.org/wiki/Recurrent_neural_network\u0027 target=\u0027_blank\u0027\u003eRecurrent neural network\u003c/a\u003e\u003cbr /\u003eA recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Recurrent_neural_network", "wikipedia_content": "A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as", "wikipedia_link": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "wikipedia_normalized": "Recurrent neural network", "wikipedia_resp_code": 200}, {"group": 3, "id": "GPT (language model)", "label": "GPT (language model)", "level": 3, "name": "GPT (Generative Pre-trained Transformer)", "node_count": 8, "processed": 2, "shape": "dot", "size": 10, "title": "8. \u003ca href=\u0027https://en.wikipedia.org/wiki/GPT_(language_model)\u0027 target=\u0027_blank\u0027\u003eGPT (language model)\u003c/a\u003e \u2192 \u003ca href=\u0027https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\u0027 target=\u0027_blank\u0027\u003eGenerative pre-trained transformer\u003c/a\u003e\u003cbr /\u003eGenerative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Generative_pre-trained_transformer", "wikipedia_content": "Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.", "wikipedia_link": "https://en.wikipedia.org/wiki/GPT_(language_model)", "wikipedia_normalized": "Generative pre-trained transformer", "wikipedia_resp_code": 200}, {"group": 3, "id": "Information retrieval", "label": "Information retrieval", "level": 3, "name": "Information retrieval", "node_count": 19, "processed": 2, "shape": "dot", "size": 10, "title": "19. \u003ca href=\u0027https://en.wikipedia.org/wiki/Information_retrieval\u0027 target=\u0027_blank\u0027\u003eInformation retrieval\u003c/a\u003e\u003cbr /\u003eInformation retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Information_retrieval", "wikipedia_content": "Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata", "wikipedia_link": "https://en.wikipedia.org/wiki/Information_retrieval", "wikipedia_normalized": "Information retrieval", "wikipedia_resp_code": 200}, {"group": 3, "id": "Machine learning", "label": "Machine learning", "level": 3, "name": "Machine learning", "node_count": 16, "processed": 2, "shape": "dot", "size": 10, "title": "16. \u003ca href=\u0027https://en.wikipedia.org/wiki/Machine_learning\u0027 target=\u0027_blank\u0027\u003eMachine learning\u003c/a\u003e\u003cbr /\u003eMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Machine_learning", "wikipedia_content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.", "wikipedia_link": "https://en.wikipedia.org/wiki/Machine_learning", "wikipedia_normalized": "Machine learning", "wikipedia_resp_code": 200}, {"group": 3, "id": "FastText", "label": "FastText", "level": 3, "name": "FastText (word embedding model)", "node_count": 11, "processed": 2, "shape": "dot", "size": 10, "title": "11. \u003ca href=\u0027https://en.wikipedia.org/wiki/FastText\u0027 target=\u0027_blank\u0027\u003eFastText\u003c/a\u003e\u003cbr /\u003efastText is a library for learning of word embeddings and text classification created by Facebook\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "FastText", "wikipedia_content": "fastText is a library for learning of word embeddings and text classification created by Facebook\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.", "wikipedia_link": "https://en.wikipedia.org/wiki/FastText", "wikipedia_normalized": "FastText", "wikipedia_resp_code": 200}, {"group": 3, "id": "Word2vec", "label": "Word2vec", "level": 3, "name": "Word2Vec (word embedding model)", "node_count": 9, "processed": 2, "shape": "dot", "size": 10, "title": "9. \u003ca href=\u0027https://en.wikipedia.org/wiki/Word2vec\u0027 target=\u0027_blank\u0027\u003eWord2vec\u003c/a\u003e\u003cbr /\u003eWord2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tom\u00e1\u0161 Mikolov and colleagues at Google and published in 2013.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Word2vec", "wikipedia_content": "Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tom\u00e1\u0161 Mikolov and colleagues at Google and published in 2013.", "wikipedia_link": "https://en.wikipedia.org/wiki/Word2vec", "wikipedia_normalized": "Word2vec", "wikipedia_resp_code": 200}, {"group": 3, "id": "ELMo", "label": "ELMo", "level": 3, "name": "ELMo (language model)", "node_count": 10, "processed": 2, "shape": "dot", "size": 10, "title": "10. \u003ca href=\u0027https://en.wikipedia.org/wiki/ELMo\u0027 target=\u0027_blank\u0027\u003eELMo\u003c/a\u003e\u003cbr /\u003eELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \"bank\" in \"river bank\" and \"bank balance\".\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "ELMo", "wikipedia_content": "ELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \"bank\" in \"river bank\" and \"bank balance\".", "wikipedia_link": "https://en.wikipedia.org/wiki/ELMo", "wikipedia_normalized": "ELMo", "wikipedia_resp_code": 200}, {"group": 3, "id": "GPT-2", "label": "GPT-2", "level": 3, "name": "GPT-2", "node_count": 12, "processed": 2, "shape": "dot", "size": 10, "title": "12. \u003ca href=\u0027https://en.wikipedia.org/wiki/GPT-2\u0027 target=\u0027_blank\u0027\u003eGPT-2\u003c/a\u003e\u003cbr /\u003eGenerative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "GPT-2", "wikipedia_content": "Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.", "wikipedia_link": "https://en.wikipedia.org/wiki/GPT-2", "wikipedia_normalized": "GPT-2", "wikipedia_resp_code": 200}, {"group": 3, "id": "Artificial neural network", "label": "Artificial neural network", "level": 3, "name": "Neural networks", "node_count": 21, "processed": 2, "shape": "dot", "size": 10, "title": "21. \u003ca href=\u0027https://en.wikipedia.org/wiki/Artificial_neural_network\u0027 target=\u0027_blank\u0027\u003eArtificial neural network\u003c/a\u003e \u2192 \u003ca href=\u0027https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\u0027 target=\u0027_blank\u0027\u003eNeural network (machine learning)\u003c/a\u003e\u003cbr /\u003eIn machine learning, a neural network is a model inspired by the neuronal organization found in the biological neural networks in animal brains.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Neural_network_(machine_learning)", "wikipedia_content": "In machine learning, a neural network is a model inspired by the neuronal organization found in the biological neural networks in animal brains.", "wikipedia_link": "https://en.wikipedia.org/wiki/Artificial_neural_network", "wikipedia_normalized": "Neural network (machine learning)", "wikipedia_resp_code": 200}, {"group": 3, "id": "Artificial intelligence", "label": "Artificial intelligence", "level": 3, "name": "Artificial intelligence", "node_count": 22, "processed": 2, "shape": "dot", "size": 10, "title": "22. \u003ca href=\u0027https://en.wikipedia.org/wiki/Artificial_intelligence\u0027 target=\u0027_blank\u0027\u003eArtificial intelligence\u003c/a\u003e\u003cbr /\u003eArtificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Artificial_intelligence", "wikipedia_content": "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.", "wikipedia_link": "https://en.wikipedia.org/wiki/Artificial_intelligence", "wikipedia_normalized": "Artificial intelligence", "wikipedia_resp_code": 200}, {"group": 3, "id": "Computer vision", "label": "Computer vision", "level": 3, "name": "Computer vision", "node_count": 23, "processed": 2, "shape": "dot", "size": 10, "title": "23. \u003ca href=\u0027https://en.wikipedia.org/wiki/Computer_vision\u0027 target=\u0027_blank\u0027\u003eComputer vision\u003c/a\u003e\u003cbr /\u003eComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Computer_vision", "wikipedia_content": "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic", "wikipedia_link": "https://en.wikipedia.org/wiki/Computer_vision", "wikipedia_normalized": "Computer vision", "wikipedia_resp_code": 200}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "Large language model", "reason": "Both Large Language Models and Transformers are types of machine learning models that have gained significant attention in the field of natural language processing.", "similarity": 0.9, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "BERT is a specific large language model that has been widely used and studied in natural language processing tasks.", "similarity": 0.85, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "GPT-3 is another example of a large language model that has been developed by OpenAI and has shown impressive capabilities in generating human-like text.", "similarity": 0.85, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "Large language models are often used in natural language processing tasks such as text generation, translation, and sentiment analysis.", "similarity": 0.8, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "Large language models like GPT-3 and BERT are built using deep learning techniques, specifically neural networks with many layers.", "similarity": 0.75, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Semantic analysis", "reason": "Both involve analyzing and understanding language data to extract meaning and insights.", "similarity": 0.8, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Semantic analysis", "reason": "Both involve extracting information and insights from textual data.", "similarity": 0.7, "to": "Text mining", "width": 1}, {"arrows": "to", "from": "Semantic analysis", "reason": "Both fields deal with the computational analysis of language data.", "similarity": 0.7, "to": "Computational linguistics", "width": 1}, {"arrows": "to", "from": "T5 (text-to-text model)", "reason": "Both T5 and Transformers are machine learning models that have been widely used in natural language processing tasks.", "similarity": 0.9, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "T5 (text-to-text model)", "reason": "BERT is another popular language model based on the Transformer architecture, similar to T5.", "similarity": 0.85, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "T5 (text-to-text model)", "reason": "GPT-3 is a state-of-the-art language model that also utilizes the Transformer architecture like T5.", "similarity": 0.8, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "T5 (text-to-text model)", "reason": "XLNet is another language model that is based on the Transformer architecture, similar to T5.", "similarity": 0.75, "to": "XLNet", "width": 1}, {"arrows": "to", "from": "T5 (text-to-text model)", "reason": "RoBERTa is a variant of BERT that also uses the Transformer architecture, making it similar to T5.", "similarity": 0.7, "to": "RoBERTa", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "Both are machine learning models that involve learning from data and making predictions.", "similarity": 0.9, "to": "Neural network", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "Both involve complex neural network architectures and are used for various machine learning tasks.", "similarity": 0.85, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "RNNs are a type of neural network that can process sequential data, similar to how Transformers can handle sequences.", "similarity": 0.8, "to": "Recurrent neural network", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "BERT is a transformer-based language model that has gained popularity for various natural language processing tasks.", "similarity": 0.75, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "GPT is another example of a transformer-based language model known for its generative capabilities.", "similarity": 0.7, "to": "GPT (language model)", "width": 1}, {"arrows": "to", "from": "Text mining", "reason": "Both involve extracting meaningful information from textual data.", "similarity": 0.9, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Text mining", "reason": "Both deal with searching and retrieving relevant information from text.", "similarity": 0.85, "to": "Information retrieval", "width": 1}, {"arrows": "to", "from": "Text mining", "reason": "Text mining often utilizes machine learning algorithms for analysis.", "similarity": 0.75, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Both are subfields of artificial intelligence and often used together in NLP applications.", "similarity": 0.9, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Both fields involve the study of language and its computational aspects.", "similarity": 0.8, "to": "Computational linguistics", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Text mining is closely related to NLP as it involves extracting useful information from text.", "similarity": 0.7, "to": "Text mining", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Both fields deal with accessing and retrieving information from large datasets, often involving text.", "similarity": 0.6, "to": "Information retrieval", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Semantic analysis is a key component of NLP, focusing on understanding the meaning of text.", "similarity": 0.8, "to": "Semantic analysis", "width": 1}, {"arrows": "to", "from": "Computational linguistics", "reason": "Both fields involve the study of language and its computational aspects.", "similarity": 0.9, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Computational linguistics", "reason": "Both fields focus on retrieving relevant information from large datasets, often involving text processing.", "similarity": 0.8, "to": "Information retrieval", "width": 1}, {"arrows": "to", "from": "Computational linguistics", "reason": "Both fields involve extracting valuable insights and knowledge from textual data using computational techniques.", "similarity": 0.7, "to": "Text mining", "width": 1}, {"arrows": "to", "from": "FastText", "reason": "Both FastText and Word2Vec are popular word embedding models developed by Facebook AI Research (FAIR). They are both used for natural language processing tasks and share similarities in their underlying algorithms.", "similarity": 0.8, "to": "Word2vec", "width": 1}, {"arrows": "to", "from": "FastText", "reason": "BERT is a state-of-the-art language model developed by Google that has revolutionized natural language understanding tasks. While FastText focuses on word embeddings, BERT operates at the sentence and context level, but both are widely used in NLP.", "similarity": 0.6, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "FastText", "reason": "ELMo is another contextual word embedding model that captures word meanings based on their context in a sentence. FastText and ELMo are both used for enhancing word representations with contextual information.", "similarity": 0.6, "to": "ELMo", "width": 1}, {"arrows": "to", "from": "Information retrieval", "reason": "Text mining involves the process of deriving high-quality information from text data sources, which is closely related to the goal of information retrieval.", "similarity": 0.7, "to": "Text mining", "width": 1}, {"arrows": "to", "from": "ELMo", "reason": "Both ELMo and BERT are popular pre-trained language models based on deep learning techniques for natural language processing tasks.", "similarity": 0.9, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "ELMo", "reason": "Word2Vec is a popular word embedding technique that, like ELMo, represents words as vectors in a continuous space.", "similarity": 0.7, "to": "Word2vec", "width": 1}, {"arrows": "to", "from": "ELMo", "reason": "ELMo and Transformers share similarities in their architecture design, both utilizing self-attention mechanisms for capturing contextual information.", "similarity": 0.85, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "GPT-2", "reason": "Both GPT-2 and GPT-3 are advanced natural language processing models developed by OpenAI, with GPT-3 being a more recent and improved version.", "similarity": 0.9, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "GPT-2", "reason": "BERT is another popular natural language processing model that utilizes transformer architecture, similar to GPT-2.", "similarity": 0.8, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "GPT-2", "reason": "Both GPT-2 and Transformers are based on transformer architecture, which has been a significant advancement in natural language processing.", "similarity": 0.7, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "GPT-2", "reason": "RNNs are another type of neural network commonly used in natural language processing tasks, sharing similarities with GPT-2 in the context of sequential data processing.", "similarity": 0.6, "to": "Recurrent neural network", "width": 1}, {"arrows": "to", "from": "GPT (language model)", "reason": "Both GPT and BERT are popular language models used in natural language processing tasks.", "similarity": 0.9, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "GPT (language model)", "reason": "GPT is based on the Transformer architecture, which is also used in other language models.", "similarity": 0.8, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Both are types of neural networks used in machine learning and artificial intelligence.", "similarity": 0.95, "to": "Artificial neural network", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Deep learning often involves neural networks with multiple layers for learning representations.", "similarity": 0.85, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Both are types of neural networks that have connections feeding back into themselves.", "similarity": 0.8, "to": "Recurrent neural network", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Neural networks are often used as models in machine learning tasks.", "similarity": 0.7, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "XLNet", "reason": "Both XLNet and BERT are transformer-based language models that have achieved state-of-the-art performance on various natural language processing tasks.", "similarity": 0.9, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "XLNet", "reason": "XLNet and GPT-3 are both transformer-based language models known for their large-scale pre-training and impressive generation capabilities.", "similarity": 0.85, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "XLNet", "reason": "RoBERTa is a variant of BERT that incorporates improvements in training and data augmentation techniques, similar to XLNet\u0027s focus on enhancing pre-training methods.", "similarity": 0.8, "to": "RoBERTa", "width": 1}, {"arrows": "to", "from": "XLNet", "reason": "XLNet and T5 both belong to the transformer model family and focus on text-to-text tasks, with T5 emphasizing the unified text-to-text framework for various NLP tasks.", "similarity": 0.7, "to": "T5 (text-to-text model)", "width": 1}, {"arrows": "to", "from": "Artificial intelligence", "reason": "Both are subfields of artificial intelligence and involve algorithms that enable machines to learn from data.", "similarity": 0.9, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Artificial intelligence", "reason": "Deep learning is a subset of machine learning that focuses on neural networks and has been a significant advancement in AI.", "similarity": 0.85, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Artificial intelligence", "reason": "Both AI and NLP deal with understanding and processing human language, with NLP being a key application of AI.", "similarity": 0.8, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Artificial intelligence", "reason": "Computer vision is a field within AI that focuses on enabling machines to interpret and understand visual information.", "similarity": 0.7, "to": "Computer vision", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "Both BERT and GPT-3 are state-of-the-art language models that use deep learning techniques for natural language processing tasks.", "similarity": 0.9, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "Word2Vec and BERT are both models used in natural language processing, with Word2Vec focusing on word embeddings while BERT is a contextual language model.", "similarity": 0.8, "to": "Word2vec", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "ELMo, like BERT, is a contextual language model that captures word meaning based on the context in which the word appears.", "similarity": 0.85, "to": "ELMo", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "Both BERT and Transformer are based on the transformer architecture, which has been highly successful in natural language processing tasks.", "similarity": 0.95, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "FastText, like BERT, is a model used for word embeddings and text classification tasks, although BERT is more focused on contextual understanding.", "similarity": 0.75, "to": "FastText", "width": 1}, {"arrows": "to", "from": "Word2vec", "reason": "FastText is another word embedding technique that shares similarities with Word2vec in terms of capturing semantic relationships between words.", "similarity": 0.7, "to": "FastText", "width": 1}, {"arrows": "to", "from": "Word2vec", "reason": "BERT is a transformer-based model that also focuses on contextual word embeddings, similar to the context-aware embeddings generated by Word2vec.", "similarity": 0.6, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "Word2vec", "reason": "ELMo is a deep contextualized word representation model that, like Word2vec, aims to capture word meanings in context.", "similarity": 0.6, "to": "ELMo", "width": 1}, {"arrows": "to", "from": "Artificial neural network", "reason": "Both involve the study and development of artificial intelligence systems inspired by the structure and function of the human brain.", "similarity": 0.9, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Artificial neural network", "reason": "Artificial neural networks are a key component of machine learning algorithms.", "similarity": 0.85, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Artificial neural network", "reason": "Both are types of neural networks used in artificial intelligence applications.", "similarity": 0.8, "to": "Recurrent neural network", "width": 1}, {"arrows": "to", "from": "Computer vision", "reason": "Both fields involve the processing and analysis of data to make intelligent decisions.", "similarity": 0.9, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Computer vision", "reason": "Computer vision is a subfield of AI that focuses on enabling machines to interpret and understand visual information.", "similarity": 0.8, "to": "Artificial intelligence", "width": 1}, {"arrows": "to", "from": "Computer vision", "reason": "Both fields involve training models to learn patterns and features from data, with deep learning being a subset of machine learning.", "similarity": 0.75, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "Both are language models developed by OpenAI, with GPT-3 being a more advanced version of GPT-2.", "similarity": 0.9, "to": "GPT-2", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "BERT is another popular language model that uses transformer architecture like GPT-3.", "similarity": 0.8, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "XLNet is a language model that also utilizes transformer architecture and has similarities in its approach to language understanding.", "similarity": 0.7, "to": "XLNet", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "T5 is a versatile language model that can perform a wide range of NLP tasks, similar to the capabilities of GPT-3.", "similarity": 0.85, "to": "T5 (text-to-text model)", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "RoBERTa is a variant of BERT that has been optimized for better performance, similar to the advancements seen in GPT-3.", "similarity": 0.75, "to": "RoBERTa", "width": 1}, {"arrows": "to", "from": "Machine learning", "reason": "Both are related to the field of AI and involve algorithms that enable machines to learn from data.", "similarity": 0.9, "to": "Artificial intelligence", "width": 1}, {"arrows": "to", "from": "Machine learning", "reason": "Deep learning is a subset of machine learning that focuses on neural networks and complex algorithms for learning representations.", "similarity": 0.85, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Machine learning", "reason": "Both fields involve developing algorithms and models that enable machines to understand and generate human language, with machine learning techniques commonly used in NLP.", "similarity": 0.7, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Both deep learning and machine learning are subfields of artificial intelligence that involve training algorithms to learn patterns from data.", "similarity": 0.9, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning heavily relies on neural networks, especially deep neural networks, for learning representations from data.", "similarity": 0.85, "to": "Artificial neural network", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning is a subset of artificial intelligence that focuses on learning representations of data through neural networks.", "similarity": 0.8, "to": "Artificial intelligence", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning has been widely used in computer vision tasks such as image recognition and object detection.", "similarity": 0.75, "to": "Computer vision", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning has shown significant advancements in natural language processing tasks like language translation and sentiment analysis.", "similarity": 0.7, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "RoBERTa", "reason": "Both RoBERTa and BERT are transformer-based language models developed by Google AI, with RoBERTa being an optimized version of BERT.", "similarity": 0.9, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "RoBERTa", "reason": "Both RoBERTa and GPT-3 are state-of-the-art language models based on transformer architecture, although they differ in training objectives and methodologies.", "similarity": 0.8, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "RoBERTa", "reason": "RoBERTa and XLNet are both transformer-based language models that have achieved high performance on various natural language processing tasks.", "similarity": 0.7, "to": "XLNet", "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": true,
        "filter": [
            "physics"
        ]
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "forceAtlas2Based": {
            "avoidOverlap": 0,
            "centralGravity": 0.01,
            "damping": 0.4,
            "gravitationalConstant": -50,
            "springConstant": 0.03,
            "springLength": 100
        },
        "solver": "forceAtlas2Based",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  
                  // if this network requires displaying the configure window,
                  // put it in its div
                  options.configure["container"] = document.getElementById("config");
                  

                  network = new vis.Network(container, data, options);

                  

                  

                  
                  // make a custom popup
                      var popup = document.createElement("div");
                      popup.className = 'popup';
                      popupTimeout = null;
                      popup.addEventListener('mouseover', function () {
                          console.log(popup)
                          if (popupTimeout !== null) {
                              clearTimeout(popupTimeout);
                              popupTimeout = null;
                          }
                      });
                      popup.addEventListener('mouseout', function () {
                          if (popupTimeout === null) {
                              hidePopup();
                          }
                      });
                      container.appendChild(popup);


                      // use the popup event to show
                      network.on("showPopup", function (params) {
                          showPopup(params);
                      });

                      // use the hide event to hide it
                      network.on("hidePopup", function (params) {
                          hidePopup();
                      });

                      // hiding the popup through css
                      function hidePopup() {
                          popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);
                      }

                      // showing the popup
                      function showPopup(nodeId) {
                          // get the data from the vis.DataSet
                          var nodeData = nodes.get([nodeId]);
                          popup.innerHTML = nodeData[0].title;

                          // get the position of the node
                          var posCanvas = network.getPositions([nodeId])[nodeId];

                          // get the bounding box of the node
                          var boundingBox = network.getBoundingBox(nodeId);

                          //position tooltip:
                          posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);

                          // convert coordinates to the DOM space
                          var posDOM = network.canvasToDOM(posCanvas);

                          // Give it an offset
                          posDOM.x += 10;
                          posDOM.y -= 20;

                          // show and place the tooltip.
                          popup.style.display = 'block';
                          popup.style.top = posDOM.y + 'px';
                          popup.style.left = posDOM.x + 'px';
                      }
                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>