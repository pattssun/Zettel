<html>
    <head>
        <meta charset="utf-8">
        
            <script>function neighbourhoodHighlight(params) {
  // console.log("in nieghbourhoodhighlight");
  allNodes = nodes.get({ returnType: "Object" });
  // originalNodes = JSON.parse(JSON.stringify(allNodes));
  // if something is selected:
  if (params.nodes.length > 0) {
    highlightActive = true;
    var i, j;
    var selectedNode = params.nodes[0];
    var degrees = 2;

    // mark all nodes as hard to read.
    for (let nodeId in allNodes) {
      // nodeColors[nodeId] = allNodes[nodeId].color;
      allNodes[nodeId].color = "rgba(200,200,200,0.5)";
      if (allNodes[nodeId].hiddenLabel === undefined) {
        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }
    var connectedNodes = network.getConnectedNodes(selectedNode);
    var allConnectedNodes = [];

    // get the second degree nodes
    for (i = 1; i < degrees; i++) {
      for (j = 0; j < connectedNodes.length; j++) {
        allConnectedNodes = allConnectedNodes.concat(
          network.getConnectedNodes(connectedNodes[j])
        );
      }
    }

    // all second degree nodes get a different color and their label back
    for (i = 0; i < allConnectedNodes.length; i++) {
      // allNodes[allConnectedNodes[i]].color = "pink";
      allNodes[allConnectedNodes[i]].color = "rgba(150,150,150,0.75)";
      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[allConnectedNodes[i]].label =
          allNodes[allConnectedNodes[i]].hiddenLabel;
        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // all first degree nodes get their own color and their label back
    for (i = 0; i < connectedNodes.length; i++) {
      // allNodes[connectedNodes[i]].color = undefined;
      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];
      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {
        allNodes[connectedNodes[i]].label =
          allNodes[connectedNodes[i]].hiddenLabel;
        allNodes[connectedNodes[i]].hiddenLabel = undefined;
      }
    }

    // the main node gets its own color and its label back.
    // allNodes[selectedNode].color = undefined;
    allNodes[selectedNode].color = nodeColors[selectedNode];
    if (allNodes[selectedNode].hiddenLabel !== undefined) {
      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;
      allNodes[selectedNode].hiddenLabel = undefined;
    }
  } else if (highlightActive === true) {
    // console.log("highlightActive was true");
    // reset all nodes
    for (let nodeId in allNodes) {
      // allNodes[nodeId].color = "purple";
      allNodes[nodeId].color = nodeColors[nodeId];
      // delete allNodes[nodeId].color;
      if (allNodes[nodeId].hiddenLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;
        allNodes[nodeId].hiddenLabel = undefined;
      }
    }
    highlightActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    // console.log("Nothing was selected");
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        // console.log(allNodes[nodeId]);
        // allNodes[nodeId].color = {};
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function filterHighlight(params) {
  allNodes = nodes.get({ returnType: "Object" });
  // if something is selected:
  if (params.nodes.length > 0) {
    filterActive = true;
    let selectedNodes = params.nodes;

    // hiding all nodes and saving the label
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = true;
      if (allNodes[nodeId].savedLabel === undefined) {
        allNodes[nodeId].savedLabel = allNodes[nodeId].label;
        allNodes[nodeId].label = undefined;
      }
    }

    for (let i=0; i < selectedNodes.length; i++) {
      allNodes[selectedNodes[i]].hidden = false;
      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {
        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;
        allNodes[selectedNodes[i]].savedLabel = undefined;
      }
    }

  } else if (filterActive === true) {
    // reset all nodes
    for (let nodeId in allNodes) {
      allNodes[nodeId].hidden = false;
      if (allNodes[nodeId].savedLabel !== undefined) {
        allNodes[nodeId].label = allNodes[nodeId].savedLabel;
        allNodes[nodeId].savedLabel = undefined;
      }
    }
    filterActive = false;
  }

  // transform the object into an array
  var updateArray = [];
  if (params.nodes.length > 0) {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  } else {
    for (let nodeId in allNodes) {
      if (allNodes.hasOwnProperty(nodeId)) {
        updateArray.push(allNodes[nodeId]);
      }
    }
    nodes.update(updateArray);
  }
}

function selectNode(nodes) {
  network.selectNodes(nodes);
  neighbourhoodHighlight({ nodes: nodes });
  return nodes;
}

function selectNodes(nodes) {
  network.selectNodes(nodes);
  filterHighlight({nodes: nodes});
  return nodes;
}

function highlightFilter(filter) {
  let selectedNodes = []
  let selectedProp = filter['property']
  if (filter['item'] === 'node') {
    let allNodes = nodes.get({ returnType: "Object" });
    for (let nodeId in allNodes) {
      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {
        selectedNodes.push(nodeId)
      }
    }
  }
  else if (filter['item'] === 'edge'){
    let allEdges = edges.get({returnType: 'object'});
    // check if the selected property exists for selected edge and select the nodes connected to the edge
    for (let edge in allEdges) {
      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {
        selectedNodes.push(allEdges[edge]['from'])
        selectedNodes.push(allEdges[edge]['to'])
      }
    }
  }
  selectNodes(selectedNodes)
}</script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
            
            
            
            
            

        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 1200px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             
             #config {
                 float: left;
                 width: 400px;
                 height: 600px;
             }
             

             
             /* position absolute is important and the container has to be relative or absolute as well. */
          div.popup {
                 position:absolute;
                 top:0px;
                 left:0px;
                 display:none;
                 background-color:#f5f4ed;
                 -moz-border-radius: 3px;
                 -webkit-border-radius: 3px;
                 border-radius: 3px;
                 border: 1px solid #808074;
                 box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);
          }

          /* hide the original tooltip */
          .vis-tooltip {
            display:none;
          }
             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        
            <div id="config"></div>
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"group": 1, "id": "Large language model", "label": "Large language model", "level": 1, "name": "Large language model", "node_count": 0, "processed": 2, "shape": "dot", "size": 10, "title": "0. \u003ca href=\u0027https://en.wikipedia.org/wiki/Large_language_model\u0027 target=\u0027_blank\u0027\u003eLarge language model\u003c/a\u003e\u003cbr /\u003eA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\u003cbr /\u003e[200, G1, L1, PR]", "wikipedia_canonical": "Large_language_model", "wikipedia_content": "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.", "wikipedia_link": "https://en.wikipedia.org/wiki/Large_language_model", "wikipedia_normalized": "Large language model", "wikipedia_resp_code": 200}, {"group": 2, "id": "Transformer (machine learning model)", "label": "Transformer (machine learning model)", "level": 2, "name": "Transformer (machine learning model)", "node_count": 1, "processed": 2, "shape": "dot", "size": 10, "title": "1. \u003ca href=\u0027https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\u0027 target=\u0027_blank\u0027\u003eTransformer (machine learning model)\u003c/a\u003e \u2192 \u003ca href=\u0027https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\u0027 target=\u0027_blank\u0027\u003eTransformer (deep learning architecture)\u003c/a\u003e\u003cbr /\u003eA transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal \u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "Transformer_(deep_learning_architecture)", "wikipedia_content": "A transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal ", "wikipedia_link": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)", "wikipedia_normalized": "Transformer (deep learning architecture)", "wikipedia_resp_code": 200}, {"group": 2, "id": "BERT (language model)", "label": "BERT (language model)", "level": 2, "name": "BERT (language model)", "node_count": 2, "processed": 2, "shape": "dot", "size": 10, "title": "2. \u003ca href=\u0027https://en.wikipedia.org/wiki/BERT_(language_model)\u0027 target=\u0027_blank\u0027\u003eBERT (language model)\u003c/a\u003e\u003cbr /\u003e \nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "BERT_(language_model)", "wikipedia_content": " \nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"", "wikipedia_link": "https://en.wikipedia.org/wiki/BERT_(language_model)", "wikipedia_normalized": "BERT (language model)", "wikipedia_resp_code": 200}, {"group": 2, "id": "GPT-3", "label": "GPT-3", "level": 2, "name": "GPT-3 (language model)", "node_count": 3, "processed": 2, "shape": "dot", "size": 10, "title": "3. \u003ca href=\u0027https://en.wikipedia.org/wiki/GPT-3\u0027 target=\u0027_blank\u0027\u003eGPT-3\u003c/a\u003e\u003cbr /\u003eGenerative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "GPT-3", "wikipedia_content": "Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre", "wikipedia_link": "https://en.wikipedia.org/wiki/GPT-3", "wikipedia_normalized": "GPT-3", "wikipedia_resp_code": 200}, {"group": 2, "id": "Natural language processing", "label": "Natural language processing", "level": 2, "name": "Natural language processing", "node_count": 4, "processed": 2, "shape": "dot", "size": 10, "title": "4. \u003ca href=\u0027https://en.wikipedia.org/wiki/Natural_language_processing\u0027 target=\u0027_blank\u0027\u003eNatural language processing\u003c/a\u003e\u003cbr /\u003eNatural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the languag\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "Natural_language_processing", "wikipedia_content": "Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the languag", "wikipedia_link": "https://en.wikipedia.org/wiki/Natural_language_processing", "wikipedia_normalized": "Natural language processing", "wikipedia_resp_code": 200}, {"group": 2, "id": "Deep learning", "label": "Deep learning", "level": 2, "name": "Deep learning", "node_count": 5, "processed": 2, "shape": "dot", "size": 10, "title": "5. \u003ca href=\u0027https://en.wikipedia.org/wiki/Deep_learning\u0027 target=\u0027_blank\u0027\u003eDeep learning\u003c/a\u003e\u003cbr /\u003eDeep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\u003cbr /\u003e[200, G2, L2, PR]", "wikipedia_canonical": "Deep_learning", "wikipedia_content": "Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.", "wikipedia_link": "https://en.wikipedia.org/wiki/Deep_learning", "wikipedia_normalized": "Deep learning", "wikipedia_resp_code": 200}, {"group": 3, "id": "Neural network", "label": "Neural network", "level": 3, "name": "Neural Network", "node_count": 6, "processed": 2, "shape": "dot", "size": 10, "title": "6. \u003ca href=\u0027https://en.wikipedia.org/wiki/Neural_network\u0027 target=\u0027_blank\u0027\u003eNeural network\u003c/a\u003e\u003cbr /\u003eA neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \u2013 a population of nerve cells connected by synapses.\nIn machine learning, an arti\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Neural_network", "wikipedia_content": "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \u2013 a population of nerve cells connected by synapses.\nIn machine learning, an arti", "wikipedia_link": "https://en.wikipedia.org/wiki/Neural_network", "wikipedia_normalized": "Neural network", "wikipedia_resp_code": 200}, {"group": 3, "id": "Recurrent neural network", "label": "Recurrent neural network", "level": 3, "name": "Recurrent Neural Network (RNN)", "node_count": 7, "processed": 2, "shape": "dot", "size": 10, "title": "7. \u003ca href=\u0027https://en.wikipedia.org/wiki/Recurrent_neural_network\u0027 target=\u0027_blank\u0027\u003eRecurrent neural network\u003c/a\u003e\u003cbr /\u003eA recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Recurrent_neural_network", "wikipedia_content": "A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as", "wikipedia_link": "https://en.wikipedia.org/wiki/Recurrent_neural_network", "wikipedia_normalized": "Recurrent neural network", "wikipedia_resp_code": 200}, {"group": 3, "id": "GPT (language model)", "label": "GPT (language model)", "level": 3, "name": "GPT (Generative Pre-trained Transformer)", "node_count": 8, "processed": 2, "shape": "dot", "size": 10, "title": "8. \u003ca href=\u0027https://en.wikipedia.org/wiki/GPT_(language_model)\u0027 target=\u0027_blank\u0027\u003eGPT (language model)\u003c/a\u003e \u2192 \u003ca href=\u0027https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\u0027 target=\u0027_blank\u0027\u003eGenerative pre-trained transformer\u003c/a\u003e\u003cbr /\u003eGenerative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\u003cbr /\u003e[200, G3, L3, PR]", "wikipedia_canonical": "Generative_pre-trained_transformer", "wikipedia_content": "Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.", "wikipedia_link": "https://en.wikipedia.org/wiki/GPT_(language_model)", "wikipedia_normalized": "Generative pre-trained transformer", "wikipedia_resp_code": 200}, {"group": 3, "id": "Word2vec", "label": "Word2vec", "level": 3, "name": "Word2Vec (word embedding model)", "node_count": 9, "processed": 0, "shape": "dot", "size": 10, "title": "9. \u003ca href=\u0027https://en.wikipedia.org/wiki/Word2vec\u0027 target=\u0027_blank\u0027\u003eWord2vec\u003c/a\u003e\u003cbr /\u003eWord2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tom\u00e1\u0161 Mikolov and colleagues at Google and published in 2013.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Word2vec", "wikipedia_content": "Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tom\u00e1\u0161 Mikolov and colleagues at Google and published in 2013.", "wikipedia_link": "https://en.wikipedia.org/wiki/Word2vec", "wikipedia_normalized": "Word2vec", "wikipedia_resp_code": 200}, {"group": 3, "id": "ELMo", "label": "ELMo", "level": 3, "name": "ELMo (language model)", "node_count": 10, "processed": 0, "shape": "dot", "size": 10, "title": "10. \u003ca href=\u0027https://en.wikipedia.org/wiki/ELMo\u0027 target=\u0027_blank\u0027\u003eELMo\u003c/a\u003e\u003cbr /\u003eELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \"bank\" in \"river bank\" and \"bank balance\".\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "ELMo", "wikipedia_content": "ELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \"bank\" in \"river bank\" and \"bank balance\".", "wikipedia_link": "https://en.wikipedia.org/wiki/ELMo", "wikipedia_normalized": "ELMo", "wikipedia_resp_code": 200}, {"group": 3, "id": "FastText", "label": "FastText", "level": 3, "name": "FastText (word embedding model)", "node_count": 11, "processed": 0, "shape": "dot", "size": 10, "title": "11. \u003ca href=\u0027https://en.wikipedia.org/wiki/FastText\u0027 target=\u0027_blank\u0027\u003eFastText\u003c/a\u003e\u003cbr /\u003efastText is a library for learning of word embeddings and text classification created by Facebook\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "FastText", "wikipedia_content": "fastText is a library for learning of word embeddings and text classification created by Facebook\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.", "wikipedia_link": "https://en.wikipedia.org/wiki/FastText", "wikipedia_normalized": "FastText", "wikipedia_resp_code": 200}, {"group": 3, "id": "GPT-2", "label": "GPT-2", "level": 3, "name": "GPT-2", "node_count": 12, "processed": 0, "shape": "dot", "size": 10, "title": "12. \u003ca href=\u0027https://en.wikipedia.org/wiki/GPT-2\u0027 target=\u0027_blank\u0027\u003eGPT-2\u003c/a\u003e\u003cbr /\u003eGenerative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "GPT-2", "wikipedia_content": "Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.", "wikipedia_link": "https://en.wikipedia.org/wiki/GPT-2", "wikipedia_normalized": "GPT-2", "wikipedia_resp_code": 200}, {"group": 500, "id": "XLNet", "label": "XLNet", "level": 3, "name": "XLNet", "node_count": 13, "processed": 0, "shape": "dot", "size": 10, "title": "13. \u003ca href=\u0027https://en.wikipedia.org/wiki/XLNet\u0027 target=\u0027_blank\u0027\u003eXLNet\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L3, UN]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/XLNet", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 500, "id": "T5 (text-to-text model)", "label": "T5 (text-to-text model)", "level": 3, "name": "T5 (Text-to-Text Transfer Transformer)", "node_count": 14, "processed": 0, "shape": "dot", "size": 10, "title": "14. \u003ca href=\u0027https://en.wikipedia.org/wiki/T5_(text-to-text_model)\u0027 target=\u0027_blank\u0027\u003eT5 (text-to-text model)\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L3, UN]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/T5_(text-to-text_model)", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 500, "id": "RoBERTa", "label": "RoBERTa", "level": 3, "name": "RoBERTa", "node_count": 15, "processed": 0, "shape": "dot", "size": 10, "title": "15. \u003ca href=\u0027https://en.wikipedia.org/wiki/RoBERTa\u0027 target=\u0027_blank\u0027\u003eRoBERTa\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L3, UN]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/RoBERTa", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 3, "id": "Machine learning", "label": "Machine learning", "level": 3, "name": "Machine learning", "node_count": 16, "processed": 0, "shape": "dot", "size": 10, "title": "16. \u003ca href=\u0027https://en.wikipedia.org/wiki/Machine_learning\u0027 target=\u0027_blank\u0027\u003eMachine learning\u003c/a\u003e\u003cbr /\u003eMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Machine_learning", "wikipedia_content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.", "wikipedia_link": "https://en.wikipedia.org/wiki/Machine_learning", "wikipedia_normalized": "Machine learning", "wikipedia_resp_code": 200}, {"group": 3, "id": "Computational linguistics", "label": "Computational linguistics", "level": 3, "name": "Computational linguistics", "node_count": 17, "processed": 0, "shape": "dot", "size": 10, "title": "17. \u003ca href=\u0027https://en.wikipedia.org/wiki/Computational_linguistics\u0027 target=\u0027_blank\u0027\u003eComputational linguistics\u003c/a\u003e\u003cbr /\u003eComputational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Computational_linguistics", "wikipedia_content": "Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.", "wikipedia_link": "https://en.wikipedia.org/wiki/Computational_linguistics", "wikipedia_normalized": "Computational linguistics", "wikipedia_resp_code": 200}, {"group": 3, "id": "Text mining", "label": "Text mining", "level": 3, "name": "Text mining", "node_count": 18, "processed": 0, "shape": "dot", "size": 10, "title": "18. \u003ca href=\u0027https://en.wikipedia.org/wiki/Text_mining\u0027 target=\u0027_blank\u0027\u003eText mining\u003c/a\u003e\u003cbr /\u003eText mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Text_mining", "wikipedia_content": "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.", "wikipedia_link": "https://en.wikipedia.org/wiki/Text_mining", "wikipedia_normalized": "Text mining", "wikipedia_resp_code": 200}, {"group": 3, "id": "Information retrieval", "label": "Information retrieval", "level": 3, "name": "Information retrieval", "node_count": 19, "processed": 0, "shape": "dot", "size": 10, "title": "19. \u003ca href=\u0027https://en.wikipedia.org/wiki/Information_retrieval\u0027 target=\u0027_blank\u0027\u003eInformation retrieval\u003c/a\u003e\u003cbr /\u003eInformation retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Information_retrieval", "wikipedia_content": "Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata", "wikipedia_link": "https://en.wikipedia.org/wiki/Information_retrieval", "wikipedia_normalized": "Information retrieval", "wikipedia_resp_code": 200}, {"group": 3, "id": "Semantic analysis", "label": "Semantic analysis", "level": 3, "name": "Semantic analysis", "node_count": 20, "processed": 0, "shape": "dot", "size": 10, "title": "20. \u003ca href=\u0027https://en.wikipedia.org/wiki/Semantic_analysis\u0027 target=\u0027_blank\u0027\u003eSemantic analysis\u003c/a\u003e\u003cbr /\u003eSemantic analysis may refer to:\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Semantic_analysis", "wikipedia_content": "Semantic analysis may refer to:", "wikipedia_link": "https://en.wikipedia.org/wiki/Semantic_analysis", "wikipedia_normalized": "Semantic analysis", "wikipedia_resp_code": 200}, {"group": 3, "id": "Artificial neural network", "label": "Artificial neural network", "level": 3, "name": "Neural networks", "node_count": 21, "processed": 0, "shape": "dot", "size": 10, "title": "21. \u003ca href=\u0027https://en.wikipedia.org/wiki/Artificial_neural_network\u0027 target=\u0027_blank\u0027\u003eArtificial neural network\u003c/a\u003e \u2192 \u003ca href=\u0027https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\u0027 target=\u0027_blank\u0027\u003eNeural network (machine learning)\u003c/a\u003e\u003cbr /\u003eIn machine learning, a neural network is a model inspired by the neuronal organization found in the biological neural networks in animal brains.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Neural_network_(machine_learning)", "wikipedia_content": "In machine learning, a neural network is a model inspired by the neuronal organization found in the biological neural networks in animal brains.", "wikipedia_link": "https://en.wikipedia.org/wiki/Artificial_neural_network", "wikipedia_normalized": "Neural network (machine learning)", "wikipedia_resp_code": 200}, {"group": 3, "id": "Artificial intelligence", "label": "Artificial intelligence", "level": 3, "name": "Artificial intelligence", "node_count": 22, "processed": 0, "shape": "dot", "size": 10, "title": "22. \u003ca href=\u0027https://en.wikipedia.org/wiki/Artificial_intelligence\u0027 target=\u0027_blank\u0027\u003eArtificial intelligence\u003c/a\u003e\u003cbr /\u003eArtificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Artificial_intelligence", "wikipedia_content": "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.", "wikipedia_link": "https://en.wikipedia.org/wiki/Artificial_intelligence", "wikipedia_normalized": "Artificial intelligence", "wikipedia_resp_code": 200}, {"group": 3, "id": "Computer vision", "label": "Computer vision", "level": 3, "name": "Computer vision", "node_count": 23, "processed": 0, "shape": "dot", "size": 10, "title": "23. \u003ca href=\u0027https://en.wikipedia.org/wiki/Computer_vision\u0027 target=\u0027_blank\u0027\u003eComputer vision\u003c/a\u003e\u003cbr /\u003eComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic\u003cbr /\u003e[200, G3, L3, UN]", "wikipedia_canonical": "Computer_vision", "wikipedia_content": "Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic", "wikipedia_link": "https://en.wikipedia.org/wiki/Computer_vision", "wikipedia_normalized": "Computer vision", "wikipedia_resp_code": 200}, {"group": 4, "id": "Convolutional neural network", "label": "Convolutional neural network", "level": 4, "name": "Convolutional neural network", "node_count": 24, "processed": 0, "shape": "dot", "size": 10, "title": "24. \u003ca href=\u0027https://en.wikipedia.org/wiki/Convolutional_neural_network\u0027 target=\u0027_blank\u0027\u003eConvolutional neural network\u003c/a\u003e\u003cbr /\u003eConvolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolutio\u003cbr /\u003e[200, G4, L4, UN]", "wikipedia_canonical": "Convolutional_neural_network", "wikipedia_content": "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolutio", "wikipedia_link": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "wikipedia_normalized": "Convolutional neural network", "wikipedia_resp_code": 200}, {"group": 4, "id": "Long short-term memory", "label": "Long short-term memory", "level": 4, "name": "Long short-term memory", "node_count": 25, "processed": 0, "shape": "dot", "size": 10, "title": "25. \u003ca href=\u0027https://en.wikipedia.org/wiki/Long_short-term_memory\u0027 target=\u0027_blank\u0027\u003eLong short-term memory\u003c/a\u003e\u003cbr /\u003eLong short-term memory (LSTM) network is a recurrent neural network (RNN), aimed at dealing with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\". It is applicable to classification, processing and predicting data based on time series, such \u003cbr /\u003e[200, G4, L4, UN]", "wikipedia_canonical": "Long_short-term_memory", "wikipedia_content": "Long short-term memory (LSTM) network is a recurrent neural network (RNN), aimed at dealing with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\". It is applicable to classification, processing and predicting data based on time series, such ", "wikipedia_link": "https://en.wikipedia.org/wiki/Long_short-term_memory", "wikipedia_normalized": "Long short-term memory", "wikipedia_resp_code": 200}, {"group": 4, "id": "Gated recurrent unit", "label": "Gated recurrent unit", "level": 4, "name": "Gated recurrent unit", "node_count": 26, "processed": 0, "shape": "dot", "size": 10, "title": "26. \u003ca href=\u0027https://en.wikipedia.org/wiki/Gated_recurrent_unit\u0027 target=\u0027_blank\u0027\u003eGated recurrent unit\u003c/a\u003e\u003cbr /\u003eGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM. \nGRU\u0027s performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs showed that gat\u003cbr /\u003e[200, G4, L4, UN]", "wikipedia_canonical": "Gated_recurrent_unit", "wikipedia_content": "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM. \nGRU\u0027s performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs showed that gat", "wikipedia_link": "https://en.wikipedia.org/wiki/Gated_recurrent_unit", "wikipedia_normalized": "Gated recurrent unit", "wikipedia_resp_code": 200}, {"group": 500, "id": "Sequence model", "label": "Sequence model", "level": 4, "name": "Sequence modeling", "node_count": 27, "processed": 0, "shape": "dot", "size": 10, "title": "27. \u003ca href=\u0027https://en.wikipedia.org/wiki/Sequence_model\u0027 target=\u0027_blank\u0027\u003eSequence model\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L4, UN]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/Sequence_model", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 4, "id": "Backpropagation through time", "label": "Backpropagation through time", "level": 4, "name": "Backpropagation through time", "node_count": 28, "processed": 0, "shape": "dot", "size": 10, "title": "28. \u003ca href=\u0027https://en.wikipedia.org/wiki/Backpropagation_through_time\u0027 target=\u0027_blank\u0027\u003eBackpropagation through time\u003c/a\u003e\u003cbr /\u003eBackpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers.\u003cbr /\u003e[200, G4, L4, UN]", "wikipedia_canonical": "Backpropagation_through_time", "wikipedia_content": "Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers.", "wikipedia_link": "https://en.wikipedia.org/wiki/Backpropagation_through_time", "wikipedia_normalized": "Backpropagation through time", "wikipedia_resp_code": 200}, {"group": 4, "id": "Vanishing gradient problem", "label": "Vanishing gradient problem", "level": 4, "name": "Vanishing gradient problem", "node_count": 29, "processed": 0, "shape": "dot", "size": 10, "title": "29. \u003ca href=\u0027https://en.wikipedia.org/wiki/Vanishing_gradient_problem\u0027 target=\u0027_blank\u0027\u003eVanishing gradient problem\u003c/a\u003e\u003cbr /\u003eIn machine learning, the vanishing gradient problem is encountered when training recurrent neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural networks weights receives an update proportional to the partial derivative of the error function with respect to the current weight. The problem is that as the sequence length increases, the gradient magnitude typically is expected to decrease, slowing the training\u003cbr /\u003e[200, G4, L4, UN]", "wikipedia_canonical": "Vanishing_gradient_problem", "wikipedia_content": "In machine learning, the vanishing gradient problem is encountered when training recurrent neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural networks weights receives an update proportional to the partial derivative of the error function with respect to the current weight. The problem is that as the sequence length increases, the gradient magnitude typically is expected to decrease, slowing the training", "wikipedia_link": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "wikipedia_normalized": "Vanishing gradient problem", "wikipedia_resp_code": 200}, {"group": 500, "id": "ELMo (language model)", "label": "ELMo (language model)", "level": 4, "name": "ELMo (language model)", "node_count": 30, "processed": 0, "shape": "dot", "size": 10, "title": "30. \u003ca href=\u0027https://en.wikipedia.org/wiki/ELMo_(language_model)\u0027 target=\u0027_blank\u0027\u003eELMo (language model)\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L4, UN]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/ELMo_(language_model)", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 500, "id": "XLNet (language model)", "label": "XLNet (language model)", "level": 4, "name": "XLNet (language model)", "node_count": 31, "processed": 0, "shape": "dot", "size": 10, "title": "31. \u003ca href=\u0027https://en.wikipedia.org/wiki/XLNet_(language_model)\u0027 target=\u0027_blank\u0027\u003eXLNet (language model)\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L4, UN]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/XLNet_(language_model)", "wikipedia_normalized": "", "wikipedia_resp_code": 404}, {"group": 500, "id": "RoBERTa (language model)", "label": "RoBERTa (language model)", "level": 4, "name": "RoBERTa (language model)", "node_count": 32, "processed": 0, "shape": "dot", "size": 10, "title": "32. \u003ca href=\u0027https://en.wikipedia.org/wiki/RoBERTa_(language_model)\u0027 target=\u0027_blank\u0027\u003eRoBERTa (language model)\u003c/a\u003e\u003cbr /\u003e\u003cbr /\u003e[404, G500, L4, UN]", "wikipedia_canonical": "", "wikipedia_content": "", "wikipedia_link": "https://en.wikipedia.org/wiki/RoBERTa_(language_model)", "wikipedia_normalized": "", "wikipedia_resp_code": 404}]);
                  edges = new vis.DataSet([{"arrows": "to", "from": "Large language model", "reason": "Both Large Language Models and Transformers are types of machine learning models that have gained significant attention in the field of natural language processing.", "similarity": 0.9, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "BERT is a specific large language model that has been widely used and studied in natural language processing tasks.", "similarity": 0.85, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "GPT-3 is another example of a large language model that has been developed by OpenAI and has shown impressive capabilities in generating human-like text.", "similarity": 0.85, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "Large language models are often used in natural language processing tasks such as text generation, translation, and sentiment analysis.", "similarity": 0.8, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Large language model", "reason": "Large language models like GPT-3 and BERT are built using deep learning techniques, specifically neural networks with many layers.", "similarity": 0.75, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "Both are machine learning models that involve learning from data and making predictions.", "similarity": 0.9, "to": "Neural network", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "Both involve complex neural network architectures and are used for various machine learning tasks.", "similarity": 0.85, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "RNNs are a type of neural network that can process sequential data, similar to how Transformers can handle sequences.", "similarity": 0.8, "to": "Recurrent neural network", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "BERT is a transformer-based language model that has gained popularity for various natural language processing tasks.", "similarity": 0.75, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "Transformer (machine learning model)", "reason": "GPT is another example of a transformer-based language model known for its generative capabilities.", "similarity": 0.7, "to": "GPT (language model)", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "Both BERT and GPT-3 are state-of-the-art language models that use deep learning techniques for natural language processing tasks.", "similarity": 0.9, "to": "GPT-3", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "Word2Vec and BERT are both models used in natural language processing, with Word2Vec focusing on word embeddings while BERT is a contextual language model.", "similarity": 0.8, "to": "Word2vec", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "ELMo, like BERT, is a contextual language model that captures word meaning based on the context in which the word appears.", "similarity": 0.85, "to": "ELMo", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "Both BERT and Transformer are based on the transformer architecture, which has been highly successful in natural language processing tasks.", "similarity": 0.95, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "BERT (language model)", "reason": "FastText, like BERT, is a model used for word embeddings and text classification tasks, although BERT is more focused on contextual understanding.", "similarity": 0.75, "to": "FastText", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "Both are language models developed by OpenAI, with GPT-3 being a more advanced version of GPT-2.", "similarity": 0.9, "to": "GPT-2", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "BERT is another popular language model that uses transformer architecture like GPT-3.", "similarity": 0.8, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "XLNet is a language model that also utilizes transformer architecture and has similarities in its approach to language understanding.", "similarity": 0.7, "to": "XLNet", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "T5 is a versatile language model that can perform a wide range of NLP tasks, similar to the capabilities of GPT-3.", "similarity": 0.85, "to": "T5 (text-to-text model)", "width": 1}, {"arrows": "to", "from": "GPT-3", "reason": "RoBERTa is a variant of BERT that has been optimized for better performance, similar to the advancements seen in GPT-3.", "similarity": 0.75, "to": "RoBERTa", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Both are subfields of artificial intelligence and often used together in NLP applications.", "similarity": 0.9, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Both fields involve the study of language and its computational aspects.", "similarity": 0.8, "to": "Computational linguistics", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Text mining is closely related to NLP as it involves extracting useful information from text.", "similarity": 0.7, "to": "Text mining", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Both fields deal with accessing and retrieving information from large datasets, often involving text.", "similarity": 0.6, "to": "Information retrieval", "width": 1}, {"arrows": "to", "from": "Natural language processing", "reason": "Semantic analysis is a key component of NLP, focusing on understanding the meaning of text.", "similarity": 0.8, "to": "Semantic analysis", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Both deep learning and machine learning are subfields of artificial intelligence that involve training algorithms to learn patterns from data.", "similarity": 0.9, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning heavily relies on neural networks, especially deep neural networks, for learning representations from data.", "similarity": 0.85, "to": "Artificial neural network", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning is a subset of artificial intelligence that focuses on learning representations of data through neural networks.", "similarity": 0.8, "to": "Artificial intelligence", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning has been widely used in computer vision tasks such as image recognition and object detection.", "similarity": 0.75, "to": "Computer vision", "width": 1}, {"arrows": "to", "from": "Deep learning", "reason": "Deep learning has shown significant advancements in natural language processing tasks like language translation and sentiment analysis.", "similarity": 0.7, "to": "Natural language processing", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Both are types of neural networks used in machine learning and artificial intelligence.", "similarity": 0.95, "to": "Artificial neural network", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Deep learning often involves neural networks with multiple layers for learning representations.", "similarity": 0.85, "to": "Deep learning", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Both are types of neural networks that have connections feeding back into themselves.", "similarity": 0.8, "to": "Recurrent neural network", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Both are types of neural networks commonly used in image recognition and processing.", "similarity": 0.75, "to": "Convolutional neural network", "width": 1}, {"arrows": "to", "from": "Neural network", "reason": "Neural networks are often used as models in machine learning tasks.", "similarity": 0.7, "to": "Machine learning", "width": 1}, {"arrows": "to", "from": "Recurrent neural network", "reason": "Both are types of recurrent neural networks designed to address the vanishing gradient problem and capture long-term dependencies.", "similarity": 0.9, "to": "Long short-term memory", "width": 1}, {"arrows": "to", "from": "Recurrent neural network", "reason": "Similar to LSTM, GRU is another type of recurrent neural network that addresses the vanishing gradient problem and is used for sequence modeling.", "similarity": 0.85, "to": "Gated recurrent unit", "width": 1}, {"arrows": "to", "from": "Recurrent neural network", "reason": "Both recurrent neural networks and sequence modeling are used for tasks involving sequential data such as time series analysis, natural language processing, and speech recognition.", "similarity": 0.8, "to": "Sequence model", "width": 1}, {"arrows": "to", "from": "Recurrent neural network", "reason": "Both concepts involve training recurrent neural networks by unfolding them over time and applying backpropagation to update the weights.", "similarity": 0.75, "to": "Backpropagation through time", "width": 1}, {"arrows": "to", "from": "Recurrent neural network", "reason": "Both concepts are related as recurrent neural networks like RNNs and LSTMs face the vanishing gradient problem during training.", "similarity": 0.7, "to": "Vanishing gradient problem", "width": 1}, {"arrows": "to", "from": "GPT (language model)", "reason": "Both GPT and BERT are popular language models used in natural language processing tasks.", "similarity": 0.9, "to": "BERT (language model)", "width": 1}, {"arrows": "to", "from": "GPT (language model)", "reason": "GPT is based on the Transformer architecture, which is also used in other language models.", "similarity": 0.8, "to": "Transformer (machine learning model)", "width": 1}, {"arrows": "to", "from": "GPT (language model)", "reason": "ELMo is another popular language model that shares similarities with GPT in terms of natural language processing.", "similarity": 0.7, "to": "ELMo (language model)", "width": 1}, {"arrows": "to", "from": "GPT (language model)", "reason": "XLNet is a language model that, like GPT, is based on the Transformer architecture and used in various NLP tasks.", "similarity": 0.8, "to": "XLNet (language model)", "width": 1}, {"arrows": "to", "from": "GPT (language model)", "reason": "RoBERTa is a variant of BERT and shares similarities with GPT in terms of being a powerful language model.", "similarity": 0.85, "to": "RoBERTa (language model)", "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": true,
        "filter": [
            "physics"
        ]
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "forceAtlas2Based": {
            "avoidOverlap": 0,
            "centralGravity": 0.01,
            "damping": 0.4,
            "gravitationalConstant": -50,
            "springConstant": 0.03,
            "springLength": 100
        },
        "solver": "forceAtlas2Based",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  
                  // if this network requires displaying the configure window,
                  // put it in its div
                  options.configure["container"] = document.getElementById("config");
                  

                  network = new vis.Network(container, data, options);

                  

                  

                  
                  // make a custom popup
                      var popup = document.createElement("div");
                      popup.className = 'popup';
                      popupTimeout = null;
                      popup.addEventListener('mouseover', function () {
                          console.log(popup)
                          if (popupTimeout !== null) {
                              clearTimeout(popupTimeout);
                              popupTimeout = null;
                          }
                      });
                      popup.addEventListener('mouseout', function () {
                          if (popupTimeout === null) {
                              hidePopup();
                          }
                      });
                      container.appendChild(popup);


                      // use the popup event to show
                      network.on("showPopup", function (params) {
                          showPopup(params);
                      });

                      // use the hide event to hide it
                      network.on("hidePopup", function (params) {
                          hidePopup();
                      });

                      // hiding the popup through css
                      function hidePopup() {
                          popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);
                      }

                      // showing the popup
                      function showPopup(nodeId) {
                          // get the data from the vis.DataSet
                          var nodeData = nodes.get([nodeId]);
                          popup.innerHTML = nodeData[0].title;

                          // get the position of the node
                          var posCanvas = network.getPositions([nodeId])[nodeId];

                          // get the bounding box of the node
                          var boundingBox = network.getBoundingBox(nodeId);

                          //position tooltip:
                          posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);

                          // convert coordinates to the DOM space
                          var posDOM = network.canvasToDOM(posCanvas);

                          // Give it an offset
                          posDOM.x += 10;
                          posDOM.y -= 20;

                          // show and place the tooltip.
                          popup.style.display = 'block';
                          popup.style.top = posDOM.y + 'px';
                          popup.style.left = posDOM.x + 'px';
                      }
                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>