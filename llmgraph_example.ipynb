{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JarYwM4FxV5x"
      },
      "source": [
        "# llmgraph\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dylanhogg/llmgraph/blob/master/notebooks/llmgraph_example.ipynb)\n",
        "\n",
        "Create knowledge graphs with LLMs.\n",
        "\n",
        "https://github.com/dylanhogg/llmgraph\n",
        "\n",
        "<img src=\"https://github.com/dylanhogg/llmgraph/blob/main/docs/img/header.jpg?raw=true\" alt=\"drawing\" width=\"600px\"/>\n",
        "\n",
        "llmgraph enables you to create knowledge graphs in [GraphML](http://graphml.graphdrawing.org/), [GEXF](https://gexf.net/), and HTML formats (generated via [pyvis](https://github.com/WestHealth/pyvis)) from a given source entity Wikipedia page. The knowledge graphs are generated by extracting world knowledge from ChatGPT or other large language models (LLMs) as supported by [LiteLLM](https://github.com/BerriAI/litellm).\n",
        "\n",
        "For a background on knowledge graphs see a [youtube overview by Computerphile](https://www.youtube.com/watch?v=PZBm7M0HGzw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsrezNA9LppM"
      },
      "source": [
        "## Install llmgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Og6vjqOqxO9X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install llmgraph from pypi (https://pypi.org/project/llmgraph/)\n",
        "# (Ignore any dependency resolver issues on Google Colab, they're fine)\n",
        "%pip install llmgraph -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dwPBofNxP0z",
        "outputId": "84d515e3-6483-4e15-8412-8cd4f5f8c5a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llmgraph               1.2.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Display installed llmgraph version\n",
        "%pip list | grep llmgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpVE4aQjLraA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cN81mjiLzL5H"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import os\n",
        "import getpass\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uuq7Q3AULwXx"
      },
      "source": [
        "## Enter your OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXaylu7pzjqV",
        "outputId": "4f573ee2-b154-43c9-a555-6b1c4809a96d"
      },
      "outputs": [],
      "source": [
        "# Set OPENAI_API_KEY from user input (hidden in UI via getpass function)\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gY_-vhLL4Uf"
      },
      "source": [
        "## Run llmgraph command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-s6hACUKyeN",
        "outputId": "174747c5-0faa-4969-c85b-600b0249330b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m                                                                                \u001b[0m\n",
            "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mllmgraph [OPTIONS] ENTITY_TYPE ENTITY_WIKIPEDIA\u001b[0m\u001b[1m                        \u001b[0m\u001b[1m \u001b[0m\n",
            "\u001b[1m                                                                                \u001b[0m\n",
            " Create knowledge graphs with LLMs                                              \n",
            "                                                                                \n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Arguments \u001b[0m\u001b[2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m    entity_type           \u001b[1;33mTEXT\u001b[0m  Entity type (e.g. movie) \u001b[2m[default: None]\u001b[0m    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                  \u001b[2;31m[required]              \u001b[0m                    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m    entity_wikipedia      \u001b[1;33mTEXT\u001b[0m  Full wikipedia link to root entity          \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                  \u001b[2m[default: None]                   \u001b[0m          \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                  \u001b[2;31m[required]                        \u001b[0m          \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-entity\u001b[0m\u001b[1;36m-root\u001b[0m                            \u001b[1;33mTEXT            \u001b[0m  Optional root     \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            entity name       \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            override if       \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            different from    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            wikipedia page    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            title             \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default: None]  \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-levels\u001b[0m                                 \u001b[1;33mINTEGER         \u001b[0m  Number of levels  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            deep to construct \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            from the central  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            root entity       \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default: 2]     \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-max\u001b[0m\u001b[1;36m-sum-total-…\u001b[0m                        \u001b[1;33mINTEGER         \u001b[0m  Maximum sum of    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            tokens for graph  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            generation        \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default: 200000]\u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-output\u001b[0m\u001b[1;36m-folder\u001b[0m                          \u001b[1;33mTEXT            \u001b[0m  Folder location   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            to write outputs  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default:        \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m./_output/]      \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-llm\u001b[0m\u001b[1;36m-model\u001b[0m                              \u001b[1;33mTEXT            \u001b[0m  The model name    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default:        \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2mgpt-3.5-turbo]   \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-llm\u001b[0m\u001b[1;36m-temp\u001b[0m                               \u001b[1;33mFLOAT           \u001b[0m  LLM temperature   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            value             \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default: 0.0]   \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-llm\u001b[0m\u001b[1;36m-base-url\u001b[0m                           \u001b[1;33mTEXT            \u001b[0m  LLM will use      \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            custom base URL   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            instead of the    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            automatic one     \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default: None]  \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-allow\u001b[0m\u001b[1;36m-user-inp…\u001b[0m    \u001b[1;35m-\u001b[0m\u001b[1;35m-no\u001b[0m\u001b[1;35m-allow-user…\u001b[0m    \u001b[1;33m                \u001b[0m  Allow command     \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            line user input   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default:        \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2mallow-user-input]\u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-version\u001b[0m                                \u001b[1;33m                \u001b[0m  Display llmgraph  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            version           \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-comple…\u001b[0m                        \u001b[1;2;33m[\u001b[0m\u001b[1;33mbash\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mzsh\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mfish\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mp\u001b[0m  Install           \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                          \u001b[1;33mowershell\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mpwsh\u001b[0m\u001b[1;2;33m]\u001b[0m\u001b[1;33m \u001b[0m  completion for    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            the specified     \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            shell.            \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default: None]  \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m                        \u001b[1;2;33m[\u001b[0m\u001b[1;33mbash\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mzsh\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mfish\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mp\u001b[0m  Show completion   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                          \u001b[1;33mowershell\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mpwsh\u001b[0m\u001b[1;2;33m]\u001b[0m\u001b[1;33m \u001b[0m  for the specified \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            shell, to copy it \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            or customize the  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            installation.     \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            \u001b[2m[default: None]  \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                                   \u001b[1;33m                \u001b[0m  Show this message \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                            and exit.         \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!llmgraph --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSux5AnAxP25",
        "outputId": "093e9602-d635-4092-b4aa-bfafbc64ce6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running with \u001b[33mentity_type\u001b[0m=\u001b[32m'concepts-general'\u001b[0m, \n",
            "\u001b[33mentity_wikipedia\u001b[0m=\u001b[32m'https://en.wikipedia.org/wiki/Large_language_model'\u001b[0m, \n",
            "\u001b[33mentity_root\u001b[0m=\u001b[32m'Large language model'\u001b[0m, \u001b[33mcustom_entity_root\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlevels\u001b[0m=\u001b[1;36m3\u001b[0m, \n",
            "\u001b[33mllm_model\u001b[0m=\u001b[32m'gpt-3.5-turbo'\u001b[0m, \u001b[33mllm_temp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33moutput_folder\u001b[0m=\u001b[32m'./_output/'\u001b[0m\n",
            "Reading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Large_language_model\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mLarge language model\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m1\u001b[0m, total tokens \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m-:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Transformer_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mmachine_learning_model\u001b[0m\u001b[1;4;32m)\u001b[0m/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/BERT_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/GPT-3\u001b[0m━━\u001b[0m \u001b[32m0/3 \u001b[0m [ \u001b[33m0:00:08\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Natural_language_processing\u001b[0m \u001b[33m0:00:08\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Deep_learning\u001b[0m[32m0/3 \u001b[0m [ \u001b[33m0:00:09\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mTransformer \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mmachine learning model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m520\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Neural_network\u001b[0m━━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Recurrent_neural_network\u001b[0m━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:15\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/GPT_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mBERT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m1\u001b[0m,\u001b[1;36m021\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Word2vec\u001b[0mm\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/ELMo\u001b[0m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/FastText\u001b[0mm\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:21\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mGPT-\u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m1\u001b[0m,\u001b[1;36m568\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/GPT-2\u001b[0m\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/XLNet\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m \u001b[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/T5_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mtext-to-text_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/RoBERTa\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m[0m [ \u001b[33m0:00:28\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mNatural language processing\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m2\u001b[0m,\u001b[1;36m096\u001b[0m\u001b[1m)\u001b[0m\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Machine_learning\u001b[0m━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Computational_linguistics\u001b[0m\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Text_mining\u001b[0m━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Information_retrieval\u001b[0m━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Semantic_analysis\u001b[0m━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mDeep learning\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m2\u001b[0m,\u001b[1;36m568\u001b[0m\u001b[1m)\u001b[0m36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Artificial_neural_network\u001b[0m\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:41\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Artificial_intelligence\u001b[0m━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:42\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Computer_vision\u001b[0m━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:42\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KOutput html: 2: \u001b[35m  33%\u001b[0m \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1/3 \u001b[0m [ \u001b[33m0:00:42\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[32m'_output/concepts-general/large-language-model/concepts-general_large-language-m\u001b[0m\n",
            "\u001b[32model_v1.2.2_level2_incl_unprocessed.html'\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mNeural Network\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m3\u001b[0m,\u001b[1;36m057\u001b[0m\u001b[1m)\u001b[0m6m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Convolutional_neural_network\u001b[0mm \u001b[32m2/3 \u001b[0m [ \u001b[33m0:00:47\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mRecurrent Neural Network \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mRNN\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m3\u001b[0m,\u001b[1;36m522\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Long_short-term_memory\u001b[0m━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:00:54\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Gated_recurrent_unit\u001b[0m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:00:54\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Sequence_model\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m[33m0:00:54\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Backpropagation_through_time\u001b[0mm \u001b[32m2/3 \u001b[0m [ \u001b[33m0:00:54\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Vanishing_gradient_problem\u001b[0m[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:00:55\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mGPT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mGenerative Pre-trained Transformer\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens it/s\u001b[0m ]\n",
            "\u001b[1;36m4\u001b[0m,\u001b[1;36m044\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/ELMo_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/XLNet_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0ms\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/RoBERTa_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mWord2Vec \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mword embedding model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m4\u001b[0m,\u001b[1;36m565\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/GloVe_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mmachine_learning\u001b[0m\u001b[1;4;32m)\u001b[0m< \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Doc2vec\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m[0m [ \u001b[33m0:01:09\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mELMo \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m5\u001b[0m,\u001b[1;36m102\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/ULMFiT\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\u001b[0m [ \u001b[33m0:01:16\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mFastText \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mword embedding model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m5\u001b[0m,\u001b[1;36m652\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mGPT-\u001b[0m\u001b[1;32m2\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m6\u001b[0m,\u001b[1;36m237\u001b[0m\u001b[1m)\u001b[0m[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Word_embedding\u001b[0mm\u001b[90m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:01:29\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mXLNet\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m6\u001b[0m,\u001b[1;36m790\u001b[0m\u001b[1m)\u001b[0m[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/ALBERT_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mT5 \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mText-to-Text Transfer Transformer\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m7\u001b[0m,\u001b[1;36m377\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mRoBERTa\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m7\u001b[0m,\u001b[1;36m894\u001b[0m\u001b[1m)\u001b[0m2\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/DistilBERT\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m [ \u001b[33m0:01:48\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mMachine learning\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m8\u001b[0m,\u001b[1;36m434\u001b[0m\u001b[1m)\u001b[0m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Data_mining\u001b[0m\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:01:55\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Pattern_recognition\u001b[0mm━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:01:55\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mComputational linguistics\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m8\u001b[0m,\u001b[1;36m932\u001b[0m\u001b[1m)\u001b[0m0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Machine_translation\u001b[0mm━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Computational_semantics\u001b[0m━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:00\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mText mining\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m9\u001b[0m,\u001b[1;36m390\u001b[0m\u001b[1m)\u001b[0mm < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Text_analytics\u001b[0mm\u001b[90m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:05\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mInformation retrieval\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m9\u001b[0m,\u001b[1;36m829\u001b[0m\u001b[1m)\u001b[0m:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Information_extraction\u001b[0m━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:12\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Document_classification\u001b[0m━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:12\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Information_filtering_system\u001b[0mm \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:13\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Relevance_feedback\u001b[0m0m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:13\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mSemantic analysis\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m10\u001b[0m,\u001b[1;36m332\u001b[0m\u001b[1m)\u001b[0m6m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mNeural networks\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m10\u001b[0m,\u001b[1;36m780\u001b[0m\u001b[1m)\u001b[0m[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Neural_network_software\u001b[0m━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:23\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mArtificial intelligence\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m11\u001b[0m,\u001b[1;36m256\u001b[0m\u001b[1m)\u001b[0m:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Robotics\u001b[0m0m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:29\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KProcessing \u001b[1;32mComputer vision\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m11\u001b[0m,\u001b[1;36m743\u001b[0m\u001b[1m)\u001b[0m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Digital_image_processing\u001b[0m━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Computer_graphics\u001b[0m90m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[2KOutput html: 3: \u001b[35m  67%\u001b[0m \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m2/3 \u001b[0m [ \u001b[33m0:02:34\u001b[0m < \u001b[36m-:--:--\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[32m'_output/concepts-general/large-language-model/concepts-general_large-language-m\u001b[0m\n",
            "\u001b[32model_v1.2.2_level3_incl_unprocessed.html'\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing level 3: \u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3 \u001b[0m [ \u001b[33m0:02:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]1m? it/s\u001b[0m ]\n",
            "\u001b[?25h\n",
            "\u001b[1;32mllmgraph finished, took \u001b[0m\u001b[1;32m154.\u001b[0m\u001b[1;32m776473s.\u001b[0m\n",
            "Output written to folder \u001b[32m'_output/concepts-general/large-language-model'\u001b[0m which \n",
            "includes, for each level:\n",
            " - An html file with only processed nodes as a fully connected graph\n",
            " - An html file with both processed and extra unprocessed edge nodes\n",
            " - A .graphml file \u001b[1m(\u001b[0msee \u001b[4;94mhttp://graphml.graphdrawing.org/\u001b[0m\u001b[4;94m)\u001b[0m\n",
            " - A .gefx file, good for viewing in gephi \u001b[1m(\u001b[0msee \u001b[4;94mhttps://gexf.net/\u001b[0m and \n",
            "\u001b[4;94mhttps://gephi.org/\u001b[0m\u001b[4;94m)\u001b[0m\n",
            "\n",
            "Thank you for using llmgraph! Please consider starring the project on github: \n",
            "\u001b[4;94mhttps://github.com/dylanhogg/llmgraph\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Run llmgraph\n",
        "# Note: valid `entity_type` values are found here: https://github.com/dylanhogg/llmgraph/blob/main/llmgraph/prompts.yaml\n",
        "!llmgraph concepts-general https://en.wikipedia.org/wiki/Large_language_model --levels 3 --llm-model gpt-3.5-turbo --llm-temp 0.0 --no-allow-user-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-g6rk6L_B5"
      },
      "source": [
        "## Locate the output files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDuAtHMq9ZaX",
        "outputId": "33b02dd3-4149-402e-e98f-dd7f278bd1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.2_level3_fully_connected.html\n",
            "_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.2_level3.graphml\n"
          ]
        }
      ],
      "source": [
        "# Get list of book html files from the _output folder\n",
        "html_files = []\n",
        "graphml_files = []\n",
        "for root, dirs, files in os.walk(\"_output\"):\n",
        "  if not dirs:\n",
        "    html_files.extend([str(Path(root) / f) for f in files if f.endswith(\"fully_connected.html\")])\n",
        "    graphml_files.extend([str(Path(root) / f) for f in files if f.endswith(\".graphml\")])\n",
        "html_files = sorted(html_files)\n",
        "graphml_files = sorted(graphml_files)\n",
        "html_file = html_files[-1]\n",
        "graphml_file = graphml_files[-1]\n",
        "\n",
        "print(html_file)\n",
        "print(graphml_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z1UZdk2WJXWF"
      },
      "outputs": [],
      "source": [
        "# Uncomment these lines to download book html (or find it in the file tree on the left)\n",
        "# from google.colab import files\n",
        "# files.download(book_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFixDPuJo11k"
      },
      "source": [
        "## Display the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7egMzj8jfAV6"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hPGvtkhTlV33"
      },
      "outputs": [],
      "source": [
        "# Load graphml file\n",
        "G = nx.read_graphml(graphml_file)\n",
        "# G = nx.read_graphml(\"_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.1_level3.graphml\")\n",
        "\n",
        "# Create pyvis network for displaying\n",
        "nt = Network(height=\"800px\", width=\"100%\", directed=True, cdn_resources=\"remote\", notebook=True)\n",
        "nt.from_nx(G)\n",
        "nt.force_atlas_2based(\n",
        "    spring_strength=0.03\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "p62WN_oclV8X",
        "outputId": "49d0e7e6-4f35-4314-9219-86ae31088e88"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "    <head>\n",
              "        <meta charset=\"utf-8\">\n",
              "        \n",
              "            <script>function neighbourhoodHighlight(params) {\n",
              "  // console.log(\"in nieghbourhoodhighlight\");\n",
              "  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "  // originalNodes = JSON.parse(JSON.stringify(allNodes));\n",
              "  // if something is selected:\n",
              "  if (params.nodes.length > 0) {\n",
              "    highlightActive = true;\n",
              "    var i, j;\n",
              "    var selectedNode = params.nodes[0];\n",
              "    var degrees = 2;\n",
              "\n",
              "    // mark all nodes as hard to read.\n",
              "    for (let nodeId in allNodes) {\n",
              "      // nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "      allNodes[nodeId].color = \"rgba(200,200,200,0.5)\";\n",
              "      if (allNodes[nodeId].hiddenLabel === undefined) {\n",
              "        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;\n",
              "        allNodes[nodeId].label = undefined;\n",
              "      }\n",
              "    }\n",
              "    var connectedNodes = network.getConnectedNodes(selectedNode);\n",
              "    var allConnectedNodes = [];\n",
              "\n",
              "    // get the second degree nodes\n",
              "    for (i = 1; i < degrees; i++) {\n",
              "      for (j = 0; j < connectedNodes.length; j++) {\n",
              "        allConnectedNodes = allConnectedNodes.concat(\n",
              "          network.getConnectedNodes(connectedNodes[j])\n",
              "        );\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // all second degree nodes get a different color and their label back\n",
              "    for (i = 0; i < allConnectedNodes.length; i++) {\n",
              "      // allNodes[allConnectedNodes[i]].color = \"pink\";\n",
              "      allNodes[allConnectedNodes[i]].color = \"rgba(150,150,150,0.75)\";\n",
              "      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {\n",
              "        allNodes[allConnectedNodes[i]].label =\n",
              "          allNodes[allConnectedNodes[i]].hiddenLabel;\n",
              "        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // all first degree nodes get their own color and their label back\n",
              "    for (i = 0; i < connectedNodes.length; i++) {\n",
              "      // allNodes[connectedNodes[i]].color = undefined;\n",
              "      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];\n",
              "      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {\n",
              "        allNodes[connectedNodes[i]].label =\n",
              "          allNodes[connectedNodes[i]].hiddenLabel;\n",
              "        allNodes[connectedNodes[i]].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // the main node gets its own color and its label back.\n",
              "    // allNodes[selectedNode].color = undefined;\n",
              "    allNodes[selectedNode].color = nodeColors[selectedNode];\n",
              "    if (allNodes[selectedNode].hiddenLabel !== undefined) {\n",
              "      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;\n",
              "      allNodes[selectedNode].hiddenLabel = undefined;\n",
              "    }\n",
              "  } else if (highlightActive === true) {\n",
              "    // console.log(\"highlightActive was true\");\n",
              "    // reset all nodes\n",
              "    for (let nodeId in allNodes) {\n",
              "      // allNodes[nodeId].color = \"purple\";\n",
              "      allNodes[nodeId].color = nodeColors[nodeId];\n",
              "      // delete allNodes[nodeId].color;\n",
              "      if (allNodes[nodeId].hiddenLabel !== undefined) {\n",
              "        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;\n",
              "        allNodes[nodeId].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "    highlightActive = false;\n",
              "  }\n",
              "\n",
              "  // transform the object into an array\n",
              "  var updateArray = [];\n",
              "  if (params.nodes.length > 0) {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        // console.log(allNodes[nodeId]);\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  } else {\n",
              "    // console.log(\"Nothing was selected\");\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        // console.log(allNodes[nodeId]);\n",
              "        // allNodes[nodeId].color = {};\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  }\n",
              "}\n",
              "\n",
              "function filterHighlight(params) {\n",
              "  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "  // if something is selected:\n",
              "  if (params.nodes.length > 0) {\n",
              "    filterActive = true;\n",
              "    let selectedNodes = params.nodes;\n",
              "\n",
              "    // hiding all nodes and saving the label\n",
              "    for (let nodeId in allNodes) {\n",
              "      allNodes[nodeId].hidden = true;\n",
              "      if (allNodes[nodeId].savedLabel === undefined) {\n",
              "        allNodes[nodeId].savedLabel = allNodes[nodeId].label;\n",
              "        allNodes[nodeId].label = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    for (let i=0; i < selectedNodes.length; i++) {\n",
              "      allNodes[selectedNodes[i]].hidden = false;\n",
              "      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {\n",
              "        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;\n",
              "        allNodes[selectedNodes[i]].savedLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "  } else if (filterActive === true) {\n",
              "    // reset all nodes\n",
              "    for (let nodeId in allNodes) {\n",
              "      allNodes[nodeId].hidden = false;\n",
              "      if (allNodes[nodeId].savedLabel !== undefined) {\n",
              "        allNodes[nodeId].label = allNodes[nodeId].savedLabel;\n",
              "        allNodes[nodeId].savedLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "    filterActive = false;\n",
              "  }\n",
              "\n",
              "  // transform the object into an array\n",
              "  var updateArray = [];\n",
              "  if (params.nodes.length > 0) {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  } else {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  }\n",
              "}\n",
              "\n",
              "function selectNode(nodes) {\n",
              "  network.selectNodes(nodes);\n",
              "  neighbourhoodHighlight({ nodes: nodes });\n",
              "  return nodes;\n",
              "}\n",
              "\n",
              "function selectNodes(nodes) {\n",
              "  network.selectNodes(nodes);\n",
              "  filterHighlight({nodes: nodes});\n",
              "  return nodes;\n",
              "}\n",
              "\n",
              "function highlightFilter(filter) {\n",
              "  let selectedNodes = []\n",
              "  let selectedProp = filter['property']\n",
              "  if (filter['item'] === 'node') {\n",
              "    let allNodes = nodes.get({ returnType: \"Object\" });\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {\n",
              "        selectedNodes.push(nodeId)\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  else if (filter['item'] === 'edge'){\n",
              "    let allEdges = edges.get({returnType: 'object'});\n",
              "    // check if the selected property exists for selected edge and select the nodes connected to the edge\n",
              "    for (let edge in allEdges) {\n",
              "      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {\n",
              "        selectedNodes.push(allEdges[edge]['from'])\n",
              "        selectedNodes.push(allEdges[edge]['to'])\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  selectNodes(selectedNodes)\n",
              "}</script>\n",
              "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "\n",
              "        \n",
              "<center>\n",
              "<h1></h1>\n",
              "</center>\n",
              "\n",
              "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
              "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
              "        <link\n",
              "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
              "          rel=\"stylesheet\"\n",
              "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        />\n",
              "        <script\n",
              "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
              "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        ></script>\n",
              "\n",
              "\n",
              "        <center>\n",
              "          <h1></h1>\n",
              "        </center>\n",
              "        <style type=\"text/css\">\n",
              "\n",
              "             #mynetwork {\n",
              "                 width: 100%;\n",
              "                 height: 800px;\n",
              "                 background-color: #ffffff;\n",
              "                 border: 1px solid lightgray;\n",
              "                 position: relative;\n",
              "                 float: left;\n",
              "             }\n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "             /* position absolute is important and the container has to be relative or absolute as well. */\n",
              "          div.popup {\n",
              "                 position:absolute;\n",
              "                 top:0px;\n",
              "                 left:0px;\n",
              "                 display:none;\n",
              "                 background-color:#f5f4ed;\n",
              "                 -moz-border-radius: 3px;\n",
              "                 -webkit-border-radius: 3px;\n",
              "                 border-radius: 3px;\n",
              "                 border: 1px solid #808074;\n",
              "                 box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);\n",
              "          }\n",
              "\n",
              "          /* hide the original tooltip */\n",
              "          .vis-tooltip {\n",
              "            display:none;\n",
              "          }\n",
              "             \n",
              "        </style>\n",
              "    </head>\n",
              "\n",
              "\n",
              "    <body>\n",
              "        <div class=\"card\" style=\"width: 100%\">\n",
              "            \n",
              "            \n",
              "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
              "        </div>\n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        <script type=\"text/javascript\">\n",
              "\n",
              "              // initialize global variables.\n",
              "              var edges;\n",
              "              var nodes;\n",
              "              var allNodes;\n",
              "              var allEdges;\n",
              "              var nodeColors;\n",
              "              var originalNodes;\n",
              "              var network;\n",
              "              var container;\n",
              "              var options, data;\n",
              "              var filter = {\n",
              "                  item : '',\n",
              "                  property : '',\n",
              "                  value : []\n",
              "              };\n",
              "\n",
              "              \n",
              "\n",
              "              \n",
              "\n",
              "              // This method is responsible for drawing the graph, returns the drawn network\n",
              "              function drawGraph() {\n",
              "                  var container = document.getElementById('mynetwork');\n",
              "\n",
              "                  \n",
              "\n",
              "                  // parsing and collecting nodes and edges from the python\n",
              "                  nodes = new vis.DataSet([{\"group\": 1, \"id\": \"Large language model\", \"label\": \"Large language model\", \"level\": 1, \"name\": \"Large language model\", \"node_count\": 0, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"0. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Large_language_model\\u0027 target=\\u0027_blank\\u0027\\u003eLarge language model\\u003c/a\\u003e\\u003cbr /\\u003eA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\\u003cbr /\\u003e[200, G1, L1, PR]\", \"wikipedia_canonical\": \"Large_language_model\", \"wikipedia_content\": \"A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Large_language_model\", \"wikipedia_normalized\": \"Large language model\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Transformer (machine learning model)\", \"label\": \"Transformer (machine learning model)\", \"level\": 2, \"name\": \"Transformer (machine learning model)\", \"node_count\": 1, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"1. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (machine learning model)\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (deep learning architecture)\\u003c/a\\u003e\\u003cbr /\\u003eA transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper \\\"Attention Is All You Need\\\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal \\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Transformer_(deep_learning_architecture)\", \"wikipedia_content\": \"A transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper \\\"Attention Is All You Need\\\". Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal \", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\", \"wikipedia_normalized\": \"Transformer (deep learning architecture)\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"BERT (language model)\", \"label\": \"BERT (language model)\", \"level\": 2, \"name\": \"BERT (language model)\", \"node_count\": 2, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"2. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/BERT_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eBERT (language model)\\u003c/a\\u003e\\u003cbr /\\u003e \\nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \\\"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\\\"\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"BERT_(language_model)\", \"wikipedia_content\": \" \\nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \\\"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\\\"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/BERT_(language_model)\", \"wikipedia_normalized\": \"BERT (language model)\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"GPT-3\", \"label\": \"GPT-3\", \"level\": 2, \"name\": \"GPT-3 (language model)\", \"node_count\": 3, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"3. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GPT-3\\u0027 target=\\u0027_blank\\u0027\\u003eGPT-3\\u003c/a\\u003e\\u003cbr /\\u003eGenerative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \\\"attention\\\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"GPT-3\", \"wikipedia_content\": \"Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \\\"attention\\\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GPT-3\", \"wikipedia_normalized\": \"GPT-3\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Natural language processing\", \"label\": \"Natural language processing\", \"level\": 2, \"name\": \"Natural language processing\", \"node_count\": 4, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"4. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Natural_language_processing\\u0027 target=\\u0027_blank\\u0027\\u003eNatural language processing\\u003c/a\\u003e\\u003cbr /\\u003eNatural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \\\"understanding\\\" the contents of documents, including the contextual nuances of the languag\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Natural_language_processing\", \"wikipedia_content\": \"Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \\\"understanding\\\" the contents of documents, including the contextual nuances of the languag\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Natural_language_processing\", \"wikipedia_normalized\": \"Natural language processing\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Deep learning\", \"label\": \"Deep learning\", \"level\": 2, \"name\": \"Deep learning\", \"node_count\": 5, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"5. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Deep_learning\\u0027 target=\\u0027_blank\\u0027\\u003eDeep learning\\u003c/a\\u003e\\u003cbr /\\u003eDeep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \\\"deep\\\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Deep_learning\", \"wikipedia_content\": \"Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \\\"deep\\\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Deep_learning\", \"wikipedia_normalized\": \"Deep learning\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Neural network\", \"label\": \"Neural network\", \"level\": 3, \"name\": \"Neural Network\", \"node_count\": 6, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"6. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eNeural network\\u003c/a\\u003e\\u003cbr /\\u003eA neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \\u2013 a population of nerve cells connected by synapses.\\nIn machine learning, an arti\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Neural_network\", \"wikipedia_content\": \"A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \\u2013 a population of nerve cells connected by synapses.\\nIn machine learning, an arti\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Neural_network\", \"wikipedia_normalized\": \"Neural network\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Recurrent neural network\", \"label\": \"Recurrent neural network\", \"level\": 3, \"name\": \"Recurrent Neural Network (RNN)\", \"node_count\": 7, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"7. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Recurrent_neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eRecurrent neural network\\u003c/a\\u003e\\u003cbr /\\u003eA recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Recurrent_neural_network\", \"wikipedia_content\": \"A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Recurrent_neural_network\", \"wikipedia_normalized\": \"Recurrent neural network\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"GPT (language model)\", \"label\": \"GPT (language model)\", \"level\": 3, \"name\": \"GPT (Generative Pre-trained Transformer)\", \"node_count\": 8, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"8. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GPT_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eGPT (language model)\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\\u0027 target=\\u0027_blank\\u0027\\u003eGenerative pre-trained transformer\\u003c/a\\u003e\\u003cbr /\\u003eGenerative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Generative_pre-trained_transformer\", \"wikipedia_content\": \"Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GPT_(language_model)\", \"wikipedia_normalized\": \"Generative pre-trained transformer\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Word2vec\", \"label\": \"Word2vec\", \"level\": 3, \"name\": \"Word2Vec (word embedding model)\", \"node_count\": 9, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"9. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Word2vec\\u0027 target=\\u0027_blank\\u0027\\u003eWord2vec\\u003c/a\\u003e\\u003cbr /\\u003eWord2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tom\\u00e1\\u0161 Mikolov and colleagues at Google and published in 2013.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Word2vec\", \"wikipedia_content\": \"Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tom\\u00e1\\u0161 Mikolov and colleagues at Google and published in 2013.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Word2vec\", \"wikipedia_normalized\": \"Word2vec\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"ELMo\", \"label\": \"ELMo\", \"level\": 3, \"name\": \"ELMo (language model)\", \"node_count\": 10, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"10. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/ELMo\\u0027 target=\\u0027_blank\\u0027\\u003eELMo\\u003c/a\\u003e\\u003cbr /\\u003eELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \\\"bank\\\" in \\\"river bank\\\" and \\\"bank balance\\\".\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"ELMo\", \"wikipedia_content\": \"ELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \\\"bank\\\" in \\\"river bank\\\" and \\\"bank balance\\\".\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/ELMo\", \"wikipedia_normalized\": \"ELMo\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"FastText\", \"label\": \"FastText\", \"level\": 3, \"name\": \"FastText (word embedding model)\", \"node_count\": 11, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"11. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/FastText\\u0027 target=\\u0027_blank\\u0027\\u003eFastText\\u003c/a\\u003e\\u003cbr /\\u003efastText is a library for learning of word embeddings and text classification created by Facebook\\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"FastText\", \"wikipedia_content\": \"fastText is a library for learning of word embeddings and text classification created by Facebook\\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/FastText\", \"wikipedia_normalized\": \"FastText\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"GPT-2\", \"label\": \"GPT-2\", \"level\": 3, \"name\": \"GPT-2\", \"node_count\": 12, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"12. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GPT-2\\u0027 target=\\u0027_blank\\u0027\\u003eGPT-2\\u003c/a\\u003e\\u003cbr /\\u003eGenerative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"GPT-2\", \"wikipedia_content\": \"Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GPT-2\", \"wikipedia_normalized\": \"GPT-2\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"XLNet\", \"label\": \"XLNet\", \"level\": 3, \"name\": \"XLNet\", \"node_count\": 13, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"13. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/XLNet\\u0027 target=\\u0027_blank\\u0027\\u003eXLNet\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/XLNet\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"T5 (text-to-text model)\", \"label\": \"T5 (text-to-text model)\", \"level\": 3, \"name\": \"T5 (Text-to-Text Transfer Transformer)\", \"node_count\": 14, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"14. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/T5_(text-to-text_model)\\u0027 target=\\u0027_blank\\u0027\\u003eT5 (text-to-text model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/T5_(text-to-text_model)\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"RoBERTa\", \"label\": \"RoBERTa\", \"level\": 3, \"name\": \"RoBERTa\", \"node_count\": 15, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"15. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/RoBERTa\\u0027 target=\\u0027_blank\\u0027\\u003eRoBERTa\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/RoBERTa\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 3, \"id\": \"Machine learning\", \"label\": \"Machine learning\", \"level\": 3, \"name\": \"Machine learning\", \"node_count\": 16, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"16. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Machine_learning\\u0027 target=\\u0027_blank\\u0027\\u003eMachine learning\\u003c/a\\u003e\\u003cbr /\\u003eMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Machine_learning\", \"wikipedia_content\": \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Machine_learning\", \"wikipedia_normalized\": \"Machine learning\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Computational linguistics\", \"label\": \"Computational linguistics\", \"level\": 3, \"name\": \"Computational linguistics\", \"node_count\": 17, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"17. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computational_linguistics\\u0027 target=\\u0027_blank\\u0027\\u003eComputational linguistics\\u003c/a\\u003e\\u003cbr /\\u003eComputational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Computational_linguistics\", \"wikipedia_content\": \"Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Computational_linguistics\", \"wikipedia_normalized\": \"Computational linguistics\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Text mining\", \"label\": \"Text mining\", \"level\": 3, \"name\": \"Text mining\", \"node_count\": 18, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"18. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Text_mining\\u0027 target=\\u0027_blank\\u0027\\u003eText mining\\u003c/a\\u003e\\u003cbr /\\u003eText mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Text_mining\", \"wikipedia_content\": \"Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Text_mining\", \"wikipedia_normalized\": \"Text mining\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Information retrieval\", \"label\": \"Information retrieval\", \"level\": 3, \"name\": \"Information retrieval\", \"node_count\": 19, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"19. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Information_retrieval\\u0027 target=\\u0027_blank\\u0027\\u003eInformation retrieval\\u003c/a\\u003e\\u003cbr /\\u003eInformation retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Information_retrieval\", \"wikipedia_content\": \"Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Information_retrieval\", \"wikipedia_normalized\": \"Information retrieval\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Semantic analysis\", \"label\": \"Semantic analysis\", \"level\": 3, \"name\": \"Semantic analysis\", \"node_count\": 20, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"20. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Semantic_analysis\\u0027 target=\\u0027_blank\\u0027\\u003eSemantic analysis\\u003c/a\\u003e\\u003cbr /\\u003eSemantic analysis may refer to:\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Semantic_analysis\", \"wikipedia_content\": \"Semantic analysis may refer to:\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Semantic_analysis\", \"wikipedia_normalized\": \"Semantic analysis\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Artificial neural network\", \"label\": \"Artificial neural network\", \"level\": 3, \"name\": \"Neural networks\", \"node_count\": 21, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"21. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Artificial_neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eArtificial neural network\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\\u0027 target=\\u0027_blank\\u0027\\u003eNeural network (machine learning)\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning, a neural network is a model inspired by the neuronal organization found in the biological neural networks in animal brains.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Neural_network_(machine_learning)\", \"wikipedia_content\": \"In machine learning, a neural network is a model inspired by the neuronal organization found in the biological neural networks in animal brains.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Artificial_neural_network\", \"wikipedia_normalized\": \"Neural network (machine learning)\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Artificial intelligence\", \"label\": \"Artificial intelligence\", \"level\": 3, \"name\": \"Artificial intelligence\", \"node_count\": 22, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"22. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Artificial_intelligence\\u0027 target=\\u0027_blank\\u0027\\u003eArtificial intelligence\\u003c/a\\u003e\\u003cbr /\\u003eArtificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Artificial_intelligence\", \"wikipedia_content\": \"Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\", \"wikipedia_normalized\": \"Artificial intelligence\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Computer vision\", \"label\": \"Computer vision\", \"level\": 3, \"name\": \"Computer vision\", \"node_count\": 23, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"23. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computer_vision\\u0027 target=\\u0027_blank\\u0027\\u003eComputer vision\\u003c/a\\u003e\\u003cbr /\\u003eComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Computer_vision\", \"wikipedia_content\": \"Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Computer_vision\", \"wikipedia_normalized\": \"Computer vision\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Convolutional neural network\", \"label\": \"Convolutional neural network\", \"level\": 4, \"name\": \"Convolutional neural network\", \"node_count\": 24, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"24. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Convolutional_neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eConvolutional neural network\\u003c/a\\u003e\\u003cbr /\\u003eConvolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \\u00d7 100 pixels. However, applying cascaded convolutio\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Convolutional_neural_network\", \"wikipedia_content\": \"Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \\u00d7 100 pixels. However, applying cascaded convolutio\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Convolutional_neural_network\", \"wikipedia_normalized\": \"Convolutional neural network\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Long short-term memory\", \"label\": \"Long short-term memory\", \"level\": 4, \"name\": \"Long short-term memory\", \"node_count\": 25, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"25. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Long_short-term_memory\\u0027 target=\\u0027_blank\\u0027\\u003eLong short-term memory\\u003c/a\\u003e\\u003cbr /\\u003eLong short-term memory (LSTM) network is a recurrent neural network (RNN), aimed at dealing with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \\\"long short-term memory\\\". It is applicable to classification, processing and predicting data based on time series, such \\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Long_short-term_memory\", \"wikipedia_content\": \"Long short-term memory (LSTM) network is a recurrent neural network (RNN), aimed at dealing with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \\\"long short-term memory\\\". It is applicable to classification, processing and predicting data based on time series, such \", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Long_short-term_memory\", \"wikipedia_normalized\": \"Long short-term memory\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Gated recurrent unit\", \"label\": \"Gated recurrent unit\", \"level\": 4, \"name\": \"Gated recurrent unit\", \"node_count\": 26, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"26. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Gated_recurrent_unit\\u0027 target=\\u0027_blank\\u0027\\u003eGated recurrent unit\\u003c/a\\u003e\\u003cbr /\\u003eGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM. \\nGRU\\u0027s performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs showed that gat\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Gated_recurrent_unit\", \"wikipedia_content\": \"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM. \\nGRU\\u0027s performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs showed that gat\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Gated_recurrent_unit\", \"wikipedia_normalized\": \"Gated recurrent unit\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"Sequence model\", \"label\": \"Sequence model\", \"level\": 4, \"name\": \"Sequence modeling\", \"node_count\": 27, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"27. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Sequence_model\\u0027 target=\\u0027_blank\\u0027\\u003eSequence model\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Sequence_model\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"Backpropagation through time\", \"label\": \"Backpropagation through time\", \"level\": 4, \"name\": \"Backpropagation through time\", \"node_count\": 28, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"28. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Backpropagation_through_time\\u0027 target=\\u0027_blank\\u0027\\u003eBackpropagation through time\\u003c/a\\u003e\\u003cbr /\\u003eBackpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Backpropagation_through_time\", \"wikipedia_content\": \"Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Backpropagation_through_time\", \"wikipedia_normalized\": \"Backpropagation through time\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Vanishing gradient problem\", \"label\": \"Vanishing gradient problem\", \"level\": 4, \"name\": \"Vanishing gradient problem\", \"node_count\": 29, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"29. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Vanishing_gradient_problem\\u0027 target=\\u0027_blank\\u0027\\u003eVanishing gradient problem\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning, the vanishing gradient problem is encountered when training recurrent neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural networks weights receives an update proportional to the partial derivative of the error function with respect to the current weight. The problem is that as the sequence length increases, the gradient magnitude typically is expected to decrease, slowing the training\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Vanishing_gradient_problem\", \"wikipedia_content\": \"In machine learning, the vanishing gradient problem is encountered when training recurrent neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural networks weights receives an update proportional to the partial derivative of the error function with respect to the current weight. The problem is that as the sequence length increases, the gradient magnitude typically is expected to decrease, slowing the training\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\", \"wikipedia_normalized\": \"Vanishing gradient problem\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"ELMo (language model)\", \"label\": \"ELMo (language model)\", \"level\": 4, \"name\": \"ELMo (language model)\", \"node_count\": 30, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"30. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/ELMo_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eELMo (language model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/ELMo_(language_model)\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"XLNet (language model)\", \"label\": \"XLNet (language model)\", \"level\": 4, \"name\": \"XLNet (language model)\", \"node_count\": 31, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"31. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/XLNet_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eXLNet (language model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/XLNet_(language_model)\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"RoBERTa (language model)\", \"label\": \"RoBERTa (language model)\", \"level\": 4, \"name\": \"RoBERTa (language model)\", \"node_count\": 32, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"32. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/RoBERTa_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eRoBERTa (language model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/RoBERTa_(language_model)\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"GloVe (machine learning)\", \"label\": \"GloVe (machine learning)\", \"level\": 4, \"name\": \"GloVe (Global Vectors for Word Representation)\", \"node_count\": 33, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"33. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GloVe_(machine_learning)\\u0027 target=\\u0027_blank\\u0027\\u003eGloVe (machine learning)\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GloVe\\u0027 target=\\u0027_blank\\u0027\\u003eGloVe\\u003c/a\\u003e\\u003cbr /\\u003eGloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"GloVe\", \"wikipedia_content\": \"GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GloVe_(machine_learning)\", \"wikipedia_normalized\": \"GloVe\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"Doc2vec\", \"label\": \"Doc2vec\", \"level\": 4, \"name\": \"Doc2Vec\", \"node_count\": 34, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"34. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Doc2vec\\u0027 target=\\u0027_blank\\u0027\\u003eDoc2vec\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Doc2vec\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"ULMFiT\", \"label\": \"ULMFiT\", \"level\": 4, \"name\": \"ULMFiT (Universal Language Model Fine-tuning)\", \"node_count\": 35, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"35. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/ULMFiT\\u0027 target=\\u0027_blank\\u0027\\u003eULMFiT\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/ULMFiT\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"Word embedding\", \"label\": \"Word embedding\", \"level\": 4, \"name\": \"Word Embedding\", \"node_count\": 36, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"36. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Word_embedding\\u0027 target=\\u0027_blank\\u0027\\u003eWord embedding\\u003c/a\\u003e\\u003cbr /\\u003eIn natural language processing (NLP), a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Word_embedding\", \"wikipedia_content\": \"In natural language processing (NLP), a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Word_embedding\", \"wikipedia_normalized\": \"Word embedding\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"ALBERT (language model)\", \"label\": \"ALBERT (language model)\", \"level\": 4, \"name\": \"ALBERT (A Lite BERT for Self-supervised Learning of Language Representations)\", \"node_count\": 37, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"37. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/ALBERT_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eALBERT (language model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/ALBERT_(language_model)\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"DistilBERT\", \"label\": \"DistilBERT\", \"level\": 4, \"name\": \"DistilBERT (language model)\", \"node_count\": 38, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"38. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/DistilBERT\\u0027 target=\\u0027_blank\\u0027\\u003eDistilBERT\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/DistilBERT\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"Data mining\", \"label\": \"Data mining\", \"level\": 4, \"name\": \"Data mining\", \"node_count\": 39, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"39. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Data_mining\\u0027 target=\\u0027_blank\\u0027\\u003eData mining\\u003c/a\\u003e\\u003cbr /\\u003eData mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \\\"knowledge discovery in databases\\\" process, or KDD. Aside\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Data_mining\", \"wikipedia_content\": \"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \\\"knowledge discovery in databases\\\" process, or KDD. Aside\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Data_mining\", \"wikipedia_normalized\": \"Data mining\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Pattern recognition\", \"label\": \"Pattern recognition\", \"level\": 4, \"name\": \"Pattern recognition\", \"node_count\": 40, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"40. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Pattern_recognition\\u0027 target=\\u0027_blank\\u0027\\u003ePattern recognition\\u003c/a\\u003e\\u003cbr /\\u003ePattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent pattern. PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern r\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Pattern_recognition\", \"wikipedia_content\": \"Pattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent pattern. PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern r\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Pattern_recognition\", \"wikipedia_normalized\": \"Pattern recognition\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Machine translation\", \"label\": \"Machine translation\", \"level\": 4, \"name\": \"Machine Translation\", \"node_count\": 41, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"41. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Machine_translation\\u0027 target=\\u0027_blank\\u0027\\u003eMachine translation\\u003c/a\\u003e\\u003cbr /\\u003eMachine translation is use of either rule-based or probabilistic machine learning approaches to translation of text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Machine_translation\", \"wikipedia_content\": \"Machine translation is use of either rule-based or probabilistic machine learning approaches to translation of text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Machine_translation\", \"wikipedia_normalized\": \"Machine translation\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Computational semantics\", \"label\": \"Computational semantics\", \"level\": 4, \"name\": \"Computational Semantics\", \"node_count\": 42, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"42. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computational_semantics\\u0027 target=\\u0027_blank\\u0027\\u003eComputational semantics\\u003c/a\\u003e\\u003cbr /\\u003eComputational semantics is the study of how to automate the process of constructing and reasoning with meaning representations of natural language expressions. It consequently plays an important role in natural-language processing and computational linguistics.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Computational_semantics\", \"wikipedia_content\": \"Computational semantics is the study of how to automate the process of constructing and reasoning with meaning representations of natural language expressions. It consequently plays an important role in natural-language processing and computational linguistics.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Computational_semantics\", \"wikipedia_normalized\": \"Computational semantics\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Text analytics\", \"label\": \"Text analytics\", \"level\": 4, \"name\": \"Text Analytics\", \"node_count\": 43, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"43. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Text_analytics\\u0027 target=\\u0027_blank\\u0027\\u003eText analytics\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Text_mining\\u0027 target=\\u0027_blank\\u0027\\u003eText mining\\u003c/a\\u003e\\u003cbr /\\u003eText mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Text_mining\", \"wikipedia_content\": \"Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Text_analytics\", \"wikipedia_normalized\": \"Text mining\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Information extraction\", \"label\": \"Information extraction\", \"level\": 4, \"name\": \"Information extraction\", \"node_count\": 44, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"44. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Information_extraction\\u0027 target=\\u0027_blank\\u0027\\u003eInformation extraction\\u003c/a\\u003e\\u003cbr /\\u003eInformation extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. Typically, this involves processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Information_extraction\", \"wikipedia_content\": \"Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. Typically, this involves processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Information_extraction\", \"wikipedia_normalized\": \"Information extraction\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Document classification\", \"label\": \"Document classification\", \"level\": 4, \"name\": \"Document classification\", \"node_count\": 45, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"45. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Document_classification\\u0027 target=\\u0027_blank\\u0027\\u003eDocument classification\\u003c/a\\u003e\\u003cbr /\\u003eDocument classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \\\"manually\\\" or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is ther\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Document_classification\", \"wikipedia_content\": \"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \\\"manually\\\" or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is ther\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Document_classification\", \"wikipedia_normalized\": \"Document classification\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Information filtering system\", \"label\": \"Information filtering system\", \"level\": 4, \"name\": \"Information filtering\", \"node_count\": 46, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"46. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Information_filtering_system\\u0027 target=\\u0027_blank\\u0027\\u003eInformation filtering system\\u003c/a\\u003e\\u003cbr /\\u003e\\nAn information filtering system is a system that removes redundant or unwanted information from an information stream using (semi)automated or computerized methods prior to presentation to a human user. Its main goal is the management of the information overload and increment of the semantic signal-to-noise ratio. To do this the user\\u0027s profile is compared to some reference characteristics. These characteristics may originate from the information item or the user\\u0027s social environment.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Information_filtering_system\", \"wikipedia_content\": \"\\nAn information filtering system is a system that removes redundant or unwanted information from an information stream using (semi)automated or computerized methods prior to presentation to a human user. Its main goal is the management of the information overload and increment of the semantic signal-to-noise ratio. To do this the user\\u0027s profile is compared to some reference characteristics. These characteristics may originate from the information item or the user\\u0027s social environment.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Information_filtering_system\", \"wikipedia_normalized\": \"Information filtering system\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Relevance feedback\", \"label\": \"Relevance feedback\", \"level\": 4, \"name\": \"Relevance feedback\", \"node_count\": 47, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"47. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Relevance_feedback\\u0027 target=\\u0027_blank\\u0027\\u003eRelevance feedback\\u003c/a\\u003e\\u003cbr /\\u003eRelevance feedback is a feature of some information retrieval systems. The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query. We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or \\\"pseudo\\\" feedback.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Relevance_feedback\", \"wikipedia_content\": \"Relevance feedback is a feature of some information retrieval systems. The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query. We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or \\\"pseudo\\\" feedback.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Relevance_feedback\", \"wikipedia_normalized\": \"Relevance feedback\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Neural network software\", \"label\": \"Neural network software\", \"level\": 4, \"name\": \"Neural network software\", \"node_count\": 48, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"48. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Neural_network_software\\u0027 target=\\u0027_blank\\u0027\\u003eNeural network software\\u003c/a\\u003e\\u003cbr /\\u003eNeural network software is used to simulate, research, develop, and apply artificial neural networks, software concepts adapted from biological neural networks, and in some cases, a wider array of adaptive systems such as artificial intelligence and machine learning.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Neural_network_software\", \"wikipedia_content\": \"Neural network software is used to simulate, research, develop, and apply artificial neural networks, software concepts adapted from biological neural networks, and in some cases, a wider array of adaptive systems such as artificial intelligence and machine learning.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Neural_network_software\", \"wikipedia_normalized\": \"Neural network software\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Robotics\", \"label\": \"Robotics\", \"level\": 4, \"name\": \"Robotics\", \"node_count\": 49, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"49. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Robotics\\u0027 target=\\u0027_blank\\u0027\\u003eRobotics\\u003c/a\\u003e\\u003cbr /\\u003eRobotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Robotics\", \"wikipedia_content\": \"Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Robotics\", \"wikipedia_normalized\": \"Robotics\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Digital image processing\", \"label\": \"Digital image processing\", \"level\": 4, \"name\": \"Image processing\", \"node_count\": 50, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"50. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Digital_image_processing\\u0027 target=\\u0027_blank\\u0027\\u003eDigital image processing\\u003c/a\\u003e\\u003cbr /\\u003eDigital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions digital image processing may be modeled in the form of m\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Digital_image_processing\", \"wikipedia_content\": \"Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions digital image processing may be modeled in the form of m\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Digital_image_processing\", \"wikipedia_normalized\": \"Digital image processing\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Computer graphics\", \"label\": \"Computer graphics\", \"level\": 4, \"name\": \"Computer graphics\", \"node_count\": 51, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"51. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computer_graphics\\u0027 target=\\u0027_blank\\u0027\\u003eComputer graphics\\u003c/a\\u003e\\u003cbr /\\u003eComputer graphics deals with by generating images and art with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by c\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Computer_graphics\", \"wikipedia_content\": \"Computer graphics deals with by generating images and art with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by c\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Computer_graphics\", \"wikipedia_normalized\": \"Computer graphics\", \"wikipedia_resp_code\": 200}]);\n",
              "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Both Large Language Models and Transformers are types of machine learning models that have gained significant attention in the field of natural language processing.\", \"similarity\": 0.9, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"BERT is a specific large language model that has been widely used and studied in natural language processing tasks.\", \"similarity\": 0.85, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"GPT-3 is another example of a large language model that has been developed by OpenAI and has shown impressive capabilities in generating human-like text.\", \"similarity\": 0.85, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Large language models are often used in natural language processing tasks such as text generation, translation, and sentiment analysis.\", \"similarity\": 0.8, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Large language models like GPT-3 and BERT are built using deep learning techniques, specifically neural networks with many layers.\", \"similarity\": 0.75, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"Both are machine learning models that involve learning from data and making predictions.\", \"similarity\": 0.9, \"to\": \"Neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"Both involve complex neural network architectures and are used for various machine learning tasks.\", \"similarity\": 0.85, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"RNNs are a type of neural network that can process sequential data, similar to how Transformers can handle sequences.\", \"similarity\": 0.8, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"BERT is a transformer-based language model that has gained popularity for various natural language processing tasks.\", \"similarity\": 0.75, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"GPT is another example of a transformer-based language model known for its generative capabilities.\", \"similarity\": 0.7, \"to\": \"GPT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"Both BERT and GPT-3 are state-of-the-art language models that use deep learning techniques for natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"Word2Vec and BERT are both models used in natural language processing, with Word2Vec focusing on word embeddings while BERT is a contextual language model.\", \"similarity\": 0.8, \"to\": \"Word2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"ELMo, like BERT, is a contextual language model that captures word meaning based on the context in which the word appears.\", \"similarity\": 0.85, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"Both BERT and Transformer are based on the transformer architecture, which has been highly successful in natural language processing tasks.\", \"similarity\": 0.95, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"FastText, like BERT, is a model used for word embeddings and text classification tasks, although BERT is more focused on contextual understanding.\", \"similarity\": 0.75, \"to\": \"FastText\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"Both are language models developed by OpenAI, with GPT-3 being a more advanced version of GPT-2.\", \"similarity\": 0.9, \"to\": \"GPT-2\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"BERT is another popular language model that uses transformer architecture like GPT-3.\", \"similarity\": 0.8, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"XLNet is a language model that also utilizes transformer architecture and has similarities in its approach to language understanding.\", \"similarity\": 0.7, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"T5 is a versatile language model that can perform a wide range of NLP tasks, similar to the capabilities of GPT-3.\", \"similarity\": 0.85, \"to\": \"T5 (text-to-text model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"RoBERTa is a variant of BERT that has been optimized for better performance, similar to the advancements seen in GPT-3.\", \"similarity\": 0.75, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Both are subfields of artificial intelligence and often used together in NLP applications.\", \"similarity\": 0.9, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Both fields involve the study of language and its computational aspects.\", \"similarity\": 0.8, \"to\": \"Computational linguistics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Text mining is closely related to NLP as it involves extracting useful information from text.\", \"similarity\": 0.7, \"to\": \"Text mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Both fields deal with accessing and retrieving information from large datasets, often involving text.\", \"similarity\": 0.6, \"to\": \"Information retrieval\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Semantic analysis is a key component of NLP, focusing on understanding the meaning of text.\", \"similarity\": 0.8, \"to\": \"Semantic analysis\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Both deep learning and machine learning are subfields of artificial intelligence that involve training algorithms to learn patterns from data.\", \"similarity\": 0.9, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Deep learning heavily relies on neural networks, especially deep neural networks, for learning representations from data.\", \"similarity\": 0.85, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Deep learning is a subset of artificial intelligence that focuses on learning representations of data through neural networks.\", \"similarity\": 0.8, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Deep learning has been widely used in computer vision tasks such as image recognition and object detection.\", \"similarity\": 0.75, \"to\": \"Computer vision\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Deep learning has shown significant advancements in natural language processing tasks like language translation and sentiment analysis.\", \"similarity\": 0.7, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural network\", \"reason\": \"Both are types of neural networks used in machine learning and artificial intelligence.\", \"similarity\": 0.95, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural network\", \"reason\": \"Deep learning often involves neural networks with multiple layers for learning representations.\", \"similarity\": 0.85, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural network\", \"reason\": \"Both are types of neural networks that have connections feeding back into themselves.\", \"similarity\": 0.8, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural network\", \"reason\": \"Both are types of neural networks commonly used in image recognition and processing.\", \"similarity\": 0.75, \"to\": \"Convolutional neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural network\", \"reason\": \"Neural networks are often used as models in machine learning tasks.\", \"similarity\": 0.7, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Recurrent neural network\", \"reason\": \"Both are types of recurrent neural networks designed to address the vanishing gradient problem and capture long-term dependencies.\", \"similarity\": 0.9, \"to\": \"Long short-term memory\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Recurrent neural network\", \"reason\": \"Similar to LSTM, GRU is another type of recurrent neural network that addresses the vanishing gradient problem and is used for sequence modeling.\", \"similarity\": 0.85, \"to\": \"Gated recurrent unit\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Recurrent neural network\", \"reason\": \"Both recurrent neural networks and sequence modeling are used for tasks involving sequential data such as time series analysis, natural language processing, and speech recognition.\", \"similarity\": 0.8, \"to\": \"Sequence model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Recurrent neural network\", \"reason\": \"Both concepts involve training recurrent neural networks by unfolding them over time and applying backpropagation to update the weights.\", \"similarity\": 0.75, \"to\": \"Backpropagation through time\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Recurrent neural network\", \"reason\": \"Both concepts are related as recurrent neural networks like RNNs and LSTMs face the vanishing gradient problem during training.\", \"similarity\": 0.7, \"to\": \"Vanishing gradient problem\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"Both GPT and BERT are popular language models used in natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"GPT is based on the Transformer architecture, which is also used in other language models.\", \"similarity\": 0.8, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"ELMo is another popular language model that shares similarities with GPT in terms of natural language processing.\", \"similarity\": 0.7, \"to\": \"ELMo (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"XLNet is a language model that, like GPT, is based on the Transformer architecture and used in various NLP tasks.\", \"similarity\": 0.8, \"to\": \"XLNet (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"RoBERTa is a variant of BERT and shares similarities with GPT in terms of being a powerful language model.\", \"similarity\": 0.85, \"to\": \"RoBERTa (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"Both Word2vec and GloVe are popular algorithms used for word embeddings in natural language processing tasks.\", \"similarity\": 0.8, \"to\": \"GloVe (machine learning)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"FastText is another word embedding technique that shares similarities with Word2vec in terms of capturing semantic relationships between words.\", \"similarity\": 0.7, \"to\": \"FastText\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"BERT is a transformer-based model that also focuses on contextual word embeddings, similar to the context-aware embeddings generated by Word2vec.\", \"similarity\": 0.6, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"Doc2Vec is an extension of Word2vec that can generate document-level embeddings, making it similar to Word2vec in terms of capturing semantic meanings.\", \"similarity\": 0.7, \"to\": \"Doc2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"ELMo is a deep contextualized word representation model that, like Word2vec, aims to capture word meanings in context.\", \"similarity\": 0.6, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"Both ELMo and BERT are popular pre-trained language models based on deep learning techniques for natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"GloVe is another word embedding technique like ELMo that captures semantic relationships between words in a vector space.\", \"similarity\": 0.8, \"to\": \"GloVe (machine learning)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"Word2Vec is a popular word embedding technique that, like ELMo, represents words as vectors in a continuous space.\", \"similarity\": 0.7, \"to\": \"Word2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"ELMo and Transformers share similarities in their architecture design, both utilizing self-attention mechanisms for capturing contextual information.\", \"similarity\": 0.85, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"ULMFiT is a technique for fine-tuning pre-trained language models, similar to how ELMo can be fine-tuned for specific NLP tasks.\", \"similarity\": 0.75, \"to\": \"ULMFiT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"FastText\", \"reason\": \"Both FastText and Word2Vec are popular word embedding models developed by Facebook AI Research (FAIR). They are both used for natural language processing tasks and share similarities in their underlying algorithms.\", \"similarity\": 0.8, \"to\": \"Word2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"FastText\", \"reason\": \"GloVe is another popular word embedding model commonly used in natural language processing. Like FastText, GloVe aims to capture semantic relationships between words in a vector space.\", \"similarity\": 0.7, \"to\": \"GloVe (machine learning)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"FastText\", \"reason\": \"BERT is a state-of-the-art language model developed by Google that has revolutionized natural language understanding tasks. While FastText focuses on word embeddings, BERT operates at the sentence and context level, but both are widely used in NLP.\", \"similarity\": 0.6, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"FastText\", \"reason\": \"ELMo is another contextual word embedding model that captures word meanings based on their context in a sentence. FastText and ELMo are both used for enhancing word representations with contextual information.\", \"similarity\": 0.6, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"FastText\", \"reason\": \"Doc2Vec is an extension of Word2Vec that learns document-level embeddings. FastText and Doc2Vec are both used for capturing semantic relationships at different levels of text granularity.\", \"similarity\": 0.5, \"to\": \"Doc2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-2\", \"reason\": \"Both GPT-2 and GPT-3 are advanced natural language processing models developed by OpenAI, with GPT-3 being a more recent and improved version.\", \"similarity\": 0.9, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-2\", \"reason\": \"BERT is another popular natural language processing model that utilizes transformer architecture, similar to GPT-2.\", \"similarity\": 0.8, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-2\", \"reason\": \"Both GPT-2 and Transformers are based on transformer architecture, which has been a significant advancement in natural language processing.\", \"similarity\": 0.7, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-2\", \"reason\": \"RNNs are another type of neural network commonly used in natural language processing tasks, sharing similarities with GPT-2 in the context of sequential data processing.\", \"similarity\": 0.6, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-2\", \"reason\": \"Word embeddings are a key component in language models like GPT-2, as they help represent words in a continuous vector space, enabling better language understanding.\", \"similarity\": 0.7, \"to\": \"Word embedding\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"Both XLNet and BERT are transformer-based language models that have achieved state-of-the-art performance on various natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"XLNet and GPT-3 are both transformer-based language models known for their large-scale pre-training and impressive generation capabilities.\", \"similarity\": 0.85, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"RoBERTa is a variant of BERT that incorporates improvements in training and data augmentation techniques, similar to XLNet\\u0027s focus on enhancing pre-training methods.\", \"similarity\": 0.8, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"ALBERT, like XLNet, aims to improve the efficiency and effectiveness of pre-training transformer models through parameter reduction and other optimizations.\", \"similarity\": 0.75, \"to\": \"ALBERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"XLNet and T5 both belong to the transformer model family and focus on text-to-text tasks, with T5 emphasizing the unified text-to-text framework for various NLP tasks.\", \"similarity\": 0.7, \"to\": \"T5 (text-to-text model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text model)\", \"reason\": \"Both T5 and Transformers are machine learning models that have been widely used in natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text model)\", \"reason\": \"BERT is another popular language model based on the Transformer architecture, similar to T5.\", \"similarity\": 0.85, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text model)\", \"reason\": \"GPT-3 is a state-of-the-art language model that also utilizes the Transformer architecture like T5.\", \"similarity\": 0.8, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text model)\", \"reason\": \"XLNet is another language model that is based on the Transformer architecture, similar to T5.\", \"similarity\": 0.75, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text model)\", \"reason\": \"RoBERTa is a variant of BERT that also uses the Transformer architecture, making it similar to T5.\", \"similarity\": 0.7, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"Both RoBERTa and BERT are transformer-based language models developed by Google AI, with RoBERTa being an optimized version of BERT.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"Both RoBERTa and GPT-3 are state-of-the-art language models based on transformer architecture, although they differ in training objectives and methodologies.\", \"similarity\": 0.8, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"RoBERTa and XLNet are both transformer-based language models that have achieved high performance on various natural language processing tasks.\", \"similarity\": 0.7, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"RoBERTa and ALBERT are both transformer-based language models designed to improve upon the limitations of BERT.\", \"similarity\": 0.85, \"to\": \"ALBERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"RoBERTa and DistilBERT are both derived from BERT and aim to reduce the model size and computational cost while maintaining performance.\", \"similarity\": 0.75, \"to\": \"DistilBERT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Both are related to the field of AI and involve algorithms that enable machines to learn from data.\", \"similarity\": 0.9, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Deep learning is a subset of machine learning that focuses on neural networks and complex algorithms for learning representations.\", \"similarity\": 0.85, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Both involve extracting patterns and knowledge from large datasets, with machine learning being a key technique in data mining.\", \"similarity\": 0.8, \"to\": \"Data mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Both fields involve recognizing patterns and regularities in data, with machine learning algorithms often used for pattern recognition tasks.\", \"similarity\": 0.75, \"to\": \"Pattern recognition\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Both fields involve developing algorithms and models that enable machines to understand and generate human language, with machine learning techniques commonly used in NLP.\", \"similarity\": 0.7, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Both fields involve the study of language and its computational aspects.\", \"similarity\": 0.9, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Both fields deal with the automatic translation of languages using computational methods.\", \"similarity\": 0.85, \"to\": \"Machine translation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Both fields focus on retrieving relevant information from large datasets, often involving text processing.\", \"similarity\": 0.8, \"to\": \"Information retrieval\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Both fields explore the computational aspects of meaning in language.\", \"similarity\": 0.75, \"to\": \"Computational semantics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Both fields involve extracting valuable insights and knowledge from textual data using computational techniques.\", \"similarity\": 0.7, \"to\": \"Text mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Both involve extracting meaningful information from textual data.\", \"similarity\": 0.9, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Both deal with searching and retrieving relevant information from text.\", \"similarity\": 0.85, \"to\": \"Information retrieval\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Both involve analyzing and extracting insights from text data.\", \"similarity\": 0.8, \"to\": \"Text analytics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Text mining often utilizes machine learning algorithms for analysis.\", \"similarity\": 0.75, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Both involve extracting patterns and knowledge from large datasets.\", \"similarity\": 0.7, \"to\": \"Data mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Information retrieval\", \"reason\": \"Both Information retrieval and Information extraction are subfields of natural language processing that involve extracting relevant information from unstructured data.\", \"similarity\": 0.8, \"to\": \"Information extraction\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Information retrieval\", \"reason\": \"Text mining involves the process of deriving high-quality information from text data sources, which is closely related to the goal of information retrieval.\", \"similarity\": 0.7, \"to\": \"Text mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Information retrieval\", \"reason\": \"Document classification is a task in information retrieval that involves categorizing documents based on their content or topic.\", \"similarity\": 0.6, \"to\": \"Document classification\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Information retrieval\", \"reason\": \"Information filtering is a process that involves selecting relevant information based on user preferences or criteria, similar to the goal of information retrieval.\", \"similarity\": 0.7, \"to\": \"Information filtering system\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Information retrieval\", \"reason\": \"Relevance feedback is a technique in information retrieval where users provide feedback on the relevance of retrieved results to improve future retrieval performance.\", \"similarity\": 0.8, \"to\": \"Relevance feedback\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Semantic analysis\", \"reason\": \"Both involve analyzing and understanding language data to extract meaning and insights.\", \"similarity\": 0.8, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Semantic analysis\", \"reason\": \"Both involve extracting information and insights from textual data.\", \"similarity\": 0.7, \"to\": \"Text mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Semantic analysis\", \"reason\": \"Both involve identifying and extracting structured information from unstructured data.\", \"similarity\": 0.6, \"to\": \"Information extraction\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Semantic analysis\", \"reason\": \"Both fields deal with the computational analysis of language data.\", \"similarity\": 0.7, \"to\": \"Computational linguistics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Semantic analysis\", \"reason\": \"Both involve analyzing and deriving insights from textual data.\", \"similarity\": 0.7, \"to\": \"Text analytics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Both involve the study and development of artificial intelligence systems inspired by the structure and function of the human brain.\", \"similarity\": 0.9, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Artificial neural networks are a key component of machine learning algorithms.\", \"similarity\": 0.85, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Both are types of neural networks used in artificial intelligence applications.\", \"similarity\": 0.8, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Both are types of neural networks commonly used in image and video recognition tasks.\", \"similarity\": 0.75, \"to\": \"Convolutional neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Both involve the implementation and training of neural networks for various applications.\", \"similarity\": 0.7, \"to\": \"Neural network software\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Both are subfields of artificial intelligence and involve algorithms that enable machines to learn from data.\", \"similarity\": 0.9, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Deep learning is a subset of machine learning that focuses on neural networks and has been a significant advancement in AI.\", \"similarity\": 0.85, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Both AI and NLP deal with understanding and processing human language, with NLP being a key application of AI.\", \"similarity\": 0.8, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Robotics often involves the integration of AI technologies to enable robots to perform tasks autonomously.\", \"similarity\": 0.75, \"to\": \"Robotics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Computer vision is a field within AI that focuses on enabling machines to interpret and understand visual information.\", \"similarity\": 0.7, \"to\": \"Computer vision\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computer vision\", \"reason\": \"Both fields involve the processing and analysis of data to make intelligent decisions.\", \"similarity\": 0.9, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computer vision\", \"reason\": \"Computer vision heavily relies on image processing techniques for tasks like feature extraction and object recognition.\", \"similarity\": 0.85, \"to\": \"Digital image processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computer vision\", \"reason\": \"Computer vision is a subfield of AI that focuses on enabling machines to interpret and understand visual information.\", \"similarity\": 0.8, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computer vision\", \"reason\": \"Both fields involve training models to learn patterns and features from data, with deep learning being a subset of machine learning.\", \"similarity\": 0.75, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computer vision\", \"reason\": \"Computer vision and computer graphics both deal with visual data, with computer vision focusing on understanding and interpreting images while computer graphics focuses on creating visual content.\", \"similarity\": 0.7, \"to\": \"Computer graphics\", \"width\": 1}]);\n",
              "\n",
              "                  nodeColors = {};\n",
              "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "                  for (nodeId in allNodes) {\n",
              "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "                  }\n",
              "                  allEdges = edges.get({ returnType: \"Object\" });\n",
              "                  // adding nodes and edges to the graph\n",
              "                  data = {nodes: nodes, edges: edges};\n",
              "\n",
              "                  var options = {\n",
              "    \"configure\": {\n",
              "        \"enabled\": false\n",
              "    },\n",
              "    \"edges\": {\n",
              "        \"color\": {\n",
              "            \"inherit\": true\n",
              "        },\n",
              "        \"smooth\": {\n",
              "            \"enabled\": true,\n",
              "            \"type\": \"dynamic\"\n",
              "        }\n",
              "    },\n",
              "    \"interaction\": {\n",
              "        \"dragNodes\": true,\n",
              "        \"hideEdgesOnDrag\": false,\n",
              "        \"hideNodesOnDrag\": false\n",
              "    },\n",
              "    \"physics\": {\n",
              "        \"enabled\": true,\n",
              "        \"forceAtlas2Based\": {\n",
              "            \"avoidOverlap\": 0,\n",
              "            \"centralGravity\": 0.01,\n",
              "            \"damping\": 0.4,\n",
              "            \"gravitationalConstant\": -50,\n",
              "            \"springConstant\": 0.03,\n",
              "            \"springLength\": 100\n",
              "        },\n",
              "        \"solver\": \"forceAtlas2Based\",\n",
              "        \"stabilization\": {\n",
              "            \"enabled\": true,\n",
              "            \"fit\": true,\n",
              "            \"iterations\": 1000,\n",
              "            \"onlyDynamicEdges\": false,\n",
              "            \"updateInterval\": 50\n",
              "        }\n",
              "    }\n",
              "};\n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  network = new vis.Network(container, data, options);\n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "                  // make a custom popup\n",
              "                      var popup = document.createElement(\"div\");\n",
              "                      popup.className = 'popup';\n",
              "                      popupTimeout = null;\n",
              "                      popup.addEventListener('mouseover', function () {\n",
              "                          console.log(popup)\n",
              "                          if (popupTimeout !== null) {\n",
              "                              clearTimeout(popupTimeout);\n",
              "                              popupTimeout = null;\n",
              "                          }\n",
              "                      });\n",
              "                      popup.addEventListener('mouseout', function () {\n",
              "                          if (popupTimeout === null) {\n",
              "                              hidePopup();\n",
              "                          }\n",
              "                      });\n",
              "                      container.appendChild(popup);\n",
              "\n",
              "\n",
              "                      // use the popup event to show\n",
              "                      network.on(\"showPopup\", function (params) {\n",
              "                          showPopup(params);\n",
              "                      });\n",
              "\n",
              "                      // use the hide event to hide it\n",
              "                      network.on(\"hidePopup\", function (params) {\n",
              "                          hidePopup();\n",
              "                      });\n",
              "\n",
              "                      // hiding the popup through css\n",
              "                      function hidePopup() {\n",
              "                          popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);\n",
              "                      }\n",
              "\n",
              "                      // showing the popup\n",
              "                      function showPopup(nodeId) {\n",
              "                          // get the data from the vis.DataSet\n",
              "                          var nodeData = nodes.get([nodeId]);\n",
              "                          popup.innerHTML = nodeData[0].title;\n",
              "\n",
              "                          // get the position of the node\n",
              "                          var posCanvas = network.getPositions([nodeId])[nodeId];\n",
              "\n",
              "                          // get the bounding box of the node\n",
              "                          var boundingBox = network.getBoundingBox(nodeId);\n",
              "\n",
              "                          //position tooltip:\n",
              "                          posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);\n",
              "\n",
              "                          // convert coordinates to the DOM space\n",
              "                          var posDOM = network.canvasToDOM(posCanvas);\n",
              "\n",
              "                          // Give it an offset\n",
              "                          posDOM.x += 10;\n",
              "                          posDOM.y -= 20;\n",
              "\n",
              "                          // show and place the tooltip.\n",
              "                          popup.style.display = 'block';\n",
              "                          popup.style.top = posDOM.y + 'px';\n",
              "                          popup.style.left = posDOM.x + 'px';\n",
              "                      }\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  return network;\n",
              "\n",
              "              }\n",
              "              drawGraph();\n",
              "        </script>\n",
              "    </body>\n",
              "</html>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display pyviz network\n",
        "nt.save_graph(\"llmgraph.html\")\n",
        "IPython.display.HTML(filename=\"llmgraph.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DDdCVOn4lV-Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
